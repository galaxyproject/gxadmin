#!/bin/bash


hexdecodelines=$(cat <<EOF
import sys
import re
import binascii
query = re.compile(r'^(.*)\\\\x([0-9a-f]+)(.*)$')
for line in sys.stdin:
	asdf = '%s' % line.replace('\\\\\\\\', '\\\\')
	while True:
		m = query.match(asdf)
		if m:
			asdf = m.groups()[0]
			asdf += binascii.unhexlify(m.groups()[1])
			asdf += m.groups()[2] + '\\n'
		else:
			break
	sys.stdout.write(asdf)
EOF
)

usage(){
	if (( $# > 0 )) && [[ $1 != "safe" ]]; then
		error $@
	fi
	echo "gxadmin usage:"
	cmds="$(grep -o ' ## .*' $0 | grep -v grep | sort | sed 's/^ ## //g')"
	echo
	echo "Zergling Commands:"
	echo
	echo "$cmds" | grep '^zerg' | column -s: -t | sed 's/^/    /'
	echo
	echo "Handler Commands:"
	echo
	echo "$cmds" | grep '^handler' | column -s: -t | sed 's/^/    /'
	echo
	echo "DB Queries:"
	echo "  'query' can be exchanged with 'tsvquery' or 'csvquery' for tab- and comma-separated variants"
	echo
	echo "$cmds" | grep 'query ' | sort -k2 | column -s: -t | sed 's/^/    /'
	echo
	echo "Other:"
	echo
	echo "$cmds" | grep -v 'query ' | grep -v '^zerg' | grep -v handler | column -s: -t | sed 's/^/    /'
	echo
	echo "help / -h / --help : this message. Invoke '--help' on any subcommand for help specific to that subcommand"
	if [[ $1 == "safe" ]]; then
		exit 0;
	fi
	exit 1
}

handle_help() {
	for i in "$@"; do
		if [[ $i = --help || $i = -h ]]; then

			key="${mode}"
			if [[ ! -z "${subfunc}" ]]; then
				key="${key} ${subfunc}"
			fi

			echo "**NAME**"
			echo
			invoke_desc=$(grep "## ${key}[ :]" $0 | sed "s/.*## /gxadmin /g")
			short_desc=$(echo $invoke_desc | sed 's/.*://g')
			short_parm=$(echo $invoke_desc | sed 's/:.*//g')
			echo "${key} - ${short_desc}"
			echo
			echo "**SYNOPSIS**"
			echo
			echo $short_parm
			echo
			manual="$(cat -)"
			manual_wc="$(echo $manual | wc -c)"
			if (( manual_wc > 3 )); then
				echo "**NOTES**"
				echo
				echo "$manual"
				echo
			fi
			# exit after printing the documentation!
			exit 42
		fi
	done
}

assert_restart_lock(){
	if [ -f $HOME/.restart-lock ]; then
		echo "A restart lock exists. This means someone is probably already restarting galaxy."
		exit 3
	fi
}

error() {
	echo -e "\e[48;5;09m$@\e[m"
}

success() {
	echo -e "\e[38;5;40m$@\e[m"
}

wait_for_url() {
	url=$1; shift;

	while [ $(curl --silent $url | wc -c) -eq "0" ]; do
		sleep 5;
		echo -n '.'
	done
}

validate() {
	if [[ -z "$GALAXY_CONFIG_DIR" ]]; then
		error Please set \$GALAXY_CONFIG_DIR
		exit 1
	fi
	if [[ -z "$GALAXY_MUTABLE_CONFIG_DIR" ]]; then
		error Please set \$GALAXY_MUTABLE_CONFIG_DIR
		exit 1
	fi

	fail_count=0
	for file in ${GALAXY_CONFIG_DIR}/*.xml; do
		xmllint $file > /dev/null 2>/dev/null;
		exit_code=$?
		if (( $exit_code > 0 )); then
			fail_count=$(echo "$fail_count + 1" | bc)
			error "  FAIL: $file ($exit_code)";
		else
			success "  OK: $file";
		fi
	done;

	for file in ${GALAXY_MUTABLE_CONFIG_DIR}/*.xml; do
		xmllint $file > /dev/null 2>/dev/null;
		exit_code=$?
		if (( $exit_code > 0 )); then
			fail_count=$(echo "$fail_count + 1" | bc)
			error "  FAIL: $file ($exit_code)";
		else
			success "  OK: $file";
		fi
	done;

	if (( fail_count == 0 )); then
		success "All XML files validated"
	else
		error "XML validation failed, cancelling any actions."
		exit 1
	fi
}

zerg_swap() {
	# Ensure that we lock out other users.
	assert_restart_lock

	zerg0running=0
	supervisorctl status z0:zergling0 | grep RUNNING
	zerg0=$?
	if (( $zerg0 == 0 )); then
		echo "zerg 0 is running"
		zerg0running=1
	fi

	zerg1running=0
	supervisorctl status z1:zergling1 | grep RUNNING
	zerg1=$?
	if [ "$zerg1" -eq "0" ]; then
		echo "zerg 1 is running"
		zerg1running=1
	fi

	if [[ "$zerg1running" -eq "1" && "$zerg0running" -eq "1" ]] ; then
		error "ERROR: BOTH ARE RUNNING"
		exit 2
	fi
	if [[ "$zerg1running" -eq "0" && "$zerg0running" -eq "0" ]] ; then
		error "ERROR: NEITHER ARE RUNNING"
		exit 3
	fi

	supervisorctlstatus=`supervisorctl status | grep zergling | sed 's/\s\+/ /g'`

	echo "Ok, everything looks good."
	# And again even if everything looks good, just in case.

	StartDate=`date "+%s"`
	echo $$ > $HOME/.restart-lock

	# Ok, so now we are sure that just one is running
	if [ "$zerg1running" -eq "1" ] ; then
		to_start="z0:zergling0"
		to_stop="z1:zergling1"
		check_url="localhost:9190"
	else
		to_start="z1:zergling1"
		to_stop="z0:zergling0"
		check_url="localhost:9191"
	fi

	echo "Starting $to_start"
	supervisorctl start $to_start
	echo "$to_start should be running. Now wait patiently while it starts. This script will continue when it is ready."

	wait_for_url $check_url
	echo

	supervisorctl stop $to_stop

	FinalDate=`date "+%s"`
	timing=`date -u -d "0 $FinalDate seconds - $StartDate seconds" +"%H:%M:%S"`
	success "Swap took $timing"
	rm $HOME/.restart-lock
}

supervisor_strace() {
	task=$1
	state=$(supervisorctl status | grep $task)
	echo "$state" | grep --quiet "RUNNING"
	if (( $? > 0 )); then
		error "$task is not running"
		exit 1
	fi

	pid=$(echo $state | egrep -o 'pid ([0-9]*)' | sed 's/pid //g' | tr '\n' ' ')
	pids=$(echo "$pid"  | sed 's/^\s*//g;s/\s*$//g;s/ / -p /g')
	strace -e open,openat -p $pids
}

assert_count() {
	if (( $1 != $2 )); then
		error "$3"
		usage
		exit 1
	fi
}

assert_count_ge() {
	if (( $1 < $2 )); then
		error "$3"
		usage
		exit 1
	fi
}

query_tbl() {
	psql -c "$1" | sed 's/--+--/- | -/g'
}

query_tsv() {
	psql -c "COPY ($1) to STDOUT with CSV DELIMITER E'\t'"
}

query_csv() {
	psql -c "COPY ($1) to STDOUT with CSV DELIMITER ','"
}

query_influx() {
	arr2py=$(cat <<EOF
import sys
query_name = sys.argv[1]
fields = {x.split('=')[0]: int(x.split('=')[1]) for x in sys.argv[2].split(';')}
tags = []
if len(sys.argv) > 3 and len(sys.argv[3]) > 0:
	tags = {x.split('=')[0]: int(x.split('=')[1]) for x in sys.argv[3].split(';')}
for line in sys.stdin.read().split('\n'):
	if len(line) == 0:
		continue
	parsed = line.split(',')
	metric = query_name
	if len(tags):
		tag_data = ['%s=%s' % (k, parsed[v])  for (k, v) in tags.items()]
		metric += ',' + ','.join(tag_data)
	field_data = ['%s=%s' % (k, parsed[v])  for (k, v) in fields.items()]
	metric += ' ' + ','.join(field_data)
	print(metric)
EOF
)

	psql -c "COPY ($1) to STDOUT with CSV DELIMITER E','" | python -c "$arr2py" "$2" "$3" "$4"
}

if (( $# == 0 )); then
	usage safe
fi


##############################################################
# Functions
##############################################################

func_validate() { ## validate: validate config files
	handle_help "$@" <<-EOF
		Validate the configuration files
		**Warning**:
		- This requires you to have \`\$GALAXY_DIST\` set and to have config under \`\$GALAXY_DIST/config\`.
		- This only validates that it is well formed XML, and does **not** validate against any schemas.

		    $ gxadmin validate
		      OK: galaxy-dist/data_manager_conf.xml
		      ...
		      OK: galaxy-dist/config/tool_data_table_conf.xml
		      OK: galaxy-dist/config/tool_sheds_conf.xml
		    All XML files validated
	EOF

	validate;
}

cleanup() { ## cleanup [days]: Cleanup histories/hdas/etc for past N days (default=30)
	handle_help "$@" <<-EOF
		Cleanup histories/hdas/etc for past N days using the python objects-based method
	EOF

	days=30
	if (( $# > 0 )); then
		days=$1
	fi

	# TODO(hxr): nicer syntax?
	if [[ -z "$GALAXY_ROOT" ]]; then
		error Please set \$GALAXY_ROOT
		exit 1
	fi
	if [[ -z "$GALAXY_CONFIG_FILE" ]]; then
		error Please set \$GALAXY_CONFIG_FILE
		exit 1
	fi
	if [[ -z "$GALAXY_LOG_DIR" ]]; then
		error Please set \$GALAXY_LOG_DIR
		exit 1
	fi
	run_date=$(date --rfc-3339=seconds)

	for action in {delete_userless_histories,delete_exported_histories,purge_deleted_histories,purge_deleted_hdas,delete_datasets,purge_datasets}; do
		python $GALAXY_ROOT/scripts/cleanup_datasets/pgcleanup.py \
			-c $GALAXY_CONFIG_FILE \
			-o $days \
			-l $GALAXY_LOG_DIR \
			-s $action \
			-w 128MB \
			 >> "$GALAXY_LOG_DIR/cleanup-${run_date}-${action}.log" \
			2>> "$GALAXY_LOG_DIR/cleanup-${run_date}-${action}.err";

		# Something that telegraf can consume
		ec=$?
		if (( ec == 0 )); then
			echo "cleanup_datasets,group=$action success=1"
		else
			echo "cleanup_datasets,group=$action success=0"
		fi
	done
}

func_zerg_swap() { ## zerg swap <message>: swap zerglings
	handle_help "$@" <<-EOF
	EOF

	validate
	#assert_count $# 1 "Restart now requires a message"
	zerg_swap $1
}

func_zerg_tail() { ## zerg tail: tail zergling logs
	handle_help "$@" <<-EOF
	EOF

	tail -f $GALAXY_LOG_DIR/*zerg*.log
}

zerg_strace() { ## zerg strace [0|1|pool]: swap zerglings
	handle_help "$@" <<-EOF
	EOF

	param="$1";
	shift;
	if [[ $param == "pool" ]]; then
		supervisor_strace "gx:zergpool"
	elif [[ $param == "0" ]] || [[ $param == "1" ]]; then
		supervisor_strace "zergling$param"
	else
		supervisor_strace "gx:zerg"
	fi
}

zerg() {
	subfunc="$1"; shift
	case "$subfunc" in
		swap   ) func_zerg_swap  "$@";;
		tail   ) func_zerg_tail    "$@";;
		strace ) zerg_strace "$@";;
	esac
}

handler_strace() { ## handler strace <handler_id>: Run an strace on a specific handler (to watch it load files.)
	handle_help "$@" <<-EOF
	EOF

	supervisor_strace "hd:handler$1"
}

handler_tail() { ## handler tail: tail handler logs
	handle_help "$@" <<-EOF
	EOF

	tail -f $GALAXY_LOG_DIR/handler*.log
}

handler_restart() { ## handler restart <message>: restart handlers
	handle_help "$@" <<-EOF
	EOF

	validate
	supervisorctl restart hd:
}

handler() {
	subfunc="$1"; shift
	case "$subfunc" in
		strace  ) handler_strace  "$@";;
		tail    ) handler_tail    "$@";;
		restart ) handler_restart "$@";;
	esac
}


migrate_to_sqlite() { ## migrate-tool-install-to-sqlite: Converts normal potsgres toolshed repository tables into the SQLite version
	handle_help "$@" <<-EOF
		    $ gxadmin migrate-tool-install-to-sqlite
		    Creating new sqlite database: galaxy_install.sqlite
		    Migrating tables
		      export: tool_shed_repository
		      import: tool_shed_repository
		      ...
		      export: repository_repository_dependency_association
		      import: repository_repository_dependency_association
		    Complete
	EOF

	# Export tables
	if [[ -f  galaxy_install.sqlite ]]; then
		error "galaxy_install.sqlite exists, not overwriting"
		exit 1
	fi

	success "Creating new sqlite database: galaxy_install.sqlite"
	empty_schema=$(mktemp)
	echo "
	PRAGMA foreign_keys=OFF;
	BEGIN TRANSACTION;
	CREATE TABLE migrate_version (
		repository_id VARCHAR(250) NOT NULL,
		repository_path TEXT,
		version INTEGER,
		PRIMARY KEY (repository_id)
	);
	CREATE TABLE tool_shed_repository (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed VARCHAR(255),
		name VARCHAR(255),
		description TEXT,
		owner VARCHAR(255),
		changeset_revision VARCHAR(255),
		deleted BOOLEAN,
		metadata BLOB,
		includes_datatypes BOOLEAN,
		installed_changeset_revision VARCHAR(255),
		uninstalled BOOLEAN,
		dist_to_shed BOOLEAN,
		ctx_rev VARCHAR(10),
		status VARCHAR(255),
		error_message TEXT,
		tool_shed_status BLOB,
		PRIMARY KEY (id),
		CHECK (deleted IN (0, 1))
	);
	CREATE TABLE tool_version (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_id VARCHAR(255),
		tool_shed_repository_id INTEGER,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE tool_version_association (
		id INTEGER NOT NULL,
		tool_id INTEGER NOT NULL,
		parent_id INTEGER NOT NULL,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_id) REFERENCES tool_version (id),
		FOREIGN KEY(parent_id) REFERENCES tool_version (id)
	);
	CREATE TABLE migrate_tools (
		repository_id VARCHAR(255),
		repository_path TEXT,
		version INTEGER
	);
	CREATE TABLE tool_dependency (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER NOT NULL,
		name VARCHAR(255),
		version VARCHAR(40),
		type VARCHAR(40),
		status VARCHAR(255),
		error_message TEXT,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE repository_dependency (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER NOT NULL,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE repository_repository_dependency_association (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER,
		repository_dependency_id INTEGER,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id),
		FOREIGN KEY(repository_dependency_id) REFERENCES repository_dependency (id)
	);
	CREATE INDEX ix_tool_shed_repository_name ON tool_shed_repository (name);
	CREATE INDEX ix_tool_shed_repository_deleted ON tool_shed_repository (deleted);
	CREATE INDEX ix_tool_shed_repository_tool_shed ON tool_shed_repository (tool_shed);
	CREATE INDEX ix_tool_shed_repository_changeset_revision ON tool_shed_repository (changeset_revision);
	CREATE INDEX ix_tool_shed_repository_owner ON tool_shed_repository (owner);
	CREATE INDEX ix_tool_shed_repository_includes_datatypes ON tool_shed_repository (includes_datatypes);
	CREATE INDEX ix_tool_version_tool_shed_repository_id ON tool_version (tool_shed_repository_id);
	CREATE INDEX ix_tool_version_association_tool_id ON tool_version_association (tool_id);
	CREATE INDEX ix_tool_version_association_parent_id ON tool_version_association (parent_id);
	CREATE INDEX ix_tool_dependency_tool_shed_repository_id ON tool_dependency (tool_shed_repository_id);
	CREATE INDEX ix_repository_dependency_tool_shed_repository_id ON repository_dependency (tool_shed_repository_id);
	CREATE INDEX ix_repository_repository_dependency_association_tool_shed_repository_id ON repository_repository_dependency_association (tool_shed_repository_id);
	CREATE INDEX ix_repository_repository_dependency_association_repository_dependency_id ON repository_repository_dependency_association (repository_dependency_id);
	COMMIT;
	" > ${empty_schema}
	sqlite3 galaxy_install.sqlite < ${empty_schema}
	rm ${empty_schema}

	success "Migrating tables"


	# tool_shed_repository is special :(
	table=tool_shed_repository
	success "  export: ${table}"
	export_csv=$(mktemp /tmp/tmp.gxadmin.${table}.XXXXXXXXXXX)
	psql -c "COPY (select
		id, create_time, update_time, tool_shed, name, description, owner, changeset_revision, case when deleted then 1 else 0 end, metadata, includes_datatypes, installed_changeset_revision, uninstalled, dist_to_shed, ctx_rev, status, error_message, tool_shed_status from $table) to STDOUT with CSV" > $export_csv;

	success "  import: ${table}"
	echo ".mode csv
.import ${export_csv} ${table}" | sqlite3 galaxy_install.sqlite
	if (( $? == 0 )); then
		rm ${export_csv}
	else
		error "  sql: ${export_csv}"
	fi

	sqlite3 galaxy_install.sqlite "insert into migrate_version values ('ToolShedInstall', 'lib/galaxy/model/tool_shed_install/migrate', 17)"
	# the rest are sane!
	for table in {tool_version,tool_version_association,migrate_tools,tool_dependency,repository_dependency,repository_repository_dependency_association}; do
		success "  export: ${table}"
		export_csv=$(mktemp /tmp/tmp.gxadmin.${table}.XXXXXXXXXXX)
		psql -c "COPY (select * from $table) to STDOUT with CSV" > $export_csv;

		success "  import: ${table}"
		echo ".mode csv
.import ${export_csv} ${table}" | sqlite3 galaxy_install.sqlite
		if (( $? == 0 )); then
			rm ${export_csv}
		else
			error "  sql: ${export_csv}"
		fi
	done

	success "Complete"
}

query_latest_users() { ## query latest-users: 40 recently registered users
	handle_help "$@" <<-EOF
		Returns 40 most recently registered users

		    $ gxadmin query latest-users
		     id |        create_time        | pg_size_pretty |   username    |             email
		    ----+---------------------------+----------------+---------------+--------------------------------
		      1 | 2018-10-05 11:40:42.90119 |                | helena-rasche | hxr@informatik.uni-freiburg.de
	EOF


	read -r -d '' QUERY <<-EOF
		SELECT
			id,
			create_time,
			pg_size_pretty(disk_usage) as disk_usage,
			username,
			email,
			array_to_string(ARRAY(
				select galaxy_group.name from galaxy_group where id in (
					select group_id from user_group_association where user_group_association.user_id = galaxy_user.id
				)
			), ' ') as groups,
			active
		FROM galaxy_user
		ORDER BY create_time desc
		LIMIT 40
	EOF
}

query_tool_usage() { ## query tool-usage: Counts of tool runs
	handle_help "$@" <<-EOF
		    $ gxadmin tool-usage
		                                    tool_id                                 | count
		    ------------------------------------------------------------------------+--------
		     toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/1.1.0  | 958154
		     Grouping1                                                              | 638890
		     toolshed.g2.bx.psu.edu/repos/devteam/intersect/gops_intersect_1/1.0.0  | 326959
		     toolshed.g2.bx.psu.edu/repos/devteam/get_flanks/get_flanks1/1.0.0      | 320236
		     addValue                                                               | 313470
		     toolshed.g2.bx.psu.edu/repos/devteam/join/gops_join_1/1.0.0            | 312735
		     upload1                                                                | 103595
		     toolshed.g2.bx.psu.edu/repos/rnateam/graphclust_nspdk/nspdk_sparse/9.2 |  52861
		     Filter1                                                                |  43253
	EOF

	fields="count=1"
	tags="tool_id=0"

	read -r -d '' QUERY <<-EOF
		SELECT
			j.tool_id, count(*) AS count
		FROM job j
		GROUP BY j.tool_id
		ORDER BY count DESC
	EOF
}

query_job_info() { ## query job-info <id> [id] ...: Information about a specific job
	handle_help "$@" <<-EOF
		    $ gxadmin query job-info 1
		     tool_id | state | username |        create_time         | job_runner_name | job_runner_external_id
		    ---------+-------+----------+----------------------------+-----------------+------------------------
		     upload1 | ok    | admin    | 2012-12-06 16:34:27.492711 | local:///       | 9347
	EOF

	assert_count_ge $# 1 "Missing Job ID"
	job_ids="$(echo $@ | sed 's/\s\+/,/g')"

	read -r -d '' QUERY <<-EOF
		SELECT job.id, job.tool_id, job.state, job.handler, galaxy_user.username, job.create_time, job.job_runner_name, job.job_runner_external_id
		FROM job, galaxy_user
		WHERE job.id in ($job_ids) AND job.user_id = galaxy_user.id
	EOF
}

query_datasets_created_daily() { ## query datasets-created-daily: The min/max/average/p95/p99 of total size of datasets created in a single day.
	handle_help "$@" <<-EOF
		    $ gxadmin query datasets-created-daily
		       min   |  avg   | perc_95 | perc_99 |  max
		    ---------+--------+---------+---------+-------
		     0 bytes | 338 GB | 1355 GB | 2384 GB | 42 TB
	EOF

	read -r -d '' QUERY <<-EOF
		WITH temp_queue_times AS
		(select
			date_trunc('day', create_time),
			sum(total_size)
		from dataset
		group by date_trunc
		order by date_trunc desc)
		select
			pg_size_pretty(min(sum)) as min,
			pg_size_pretty(avg(sum)) as avg,
			pg_size_pretty(percentile_cont(0.95) WITHIN GROUP (ORDER BY sum) ::bigint) as perc_95,
			pg_size_pretty(percentile_cont(0.99) WITHIN GROUP (ORDER BY sum) ::bigint) as perc_99,
			pg_size_pretty(max(sum)) as max
		from temp_queue_times
	EOF
}

query_largest_collection() { ## query largest-collection: Returns the size of the single largest collection
	handle_help "$@" <<-EOF
	EOF

	fields="count=0"
	tags=""

	read -r -d '' QUERY <<-EOF
		WITH asdf AS (
			SELECT count(*)
			FROM dataset_collection_element
			GROUP BY dataset_collection_id
			ORDER BY count desc
		)
		select max(count) as count from asdf
	EOF
}

query_queue_time() { ## query queue-time <tool_id>: The average/95%/99% a specific tool spends in queue state.
	handle_help "$@" <<-EOF
		    $ gxadmin query queue-time toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_geneBody_coverage/2.6.4.3
		           min       |     perc_95     |     perc_99     |       max
		    -----------------+-----------------+-----------------+-----------------
		     00:00:15.421457 | 00:00:55.022874 | 00:00:59.974171 | 00:01:01.211995
	EOF

	assert_count $# 1 "Missing tool ID"

	read -r -d '' QUERY <<-EOF
		WITH temp_queue_times AS
		(select
			min(a.create_time - b.create_time) as queue_time
		from
			job_state_history as a
		inner join
			job_state_history as b
		on
			(a.job_id = b.job_id)
		where
			a.job_id in (select id from job where tool_id like '%$3%' and state = 'ok' and create_time > (now() - '3 months'::interval))
			and a.state = 'running'
			and b.state = 'queued'
		group by
			a.job_id
		order by
			queue_time desc
		)
		select
			min(queue_time),
			percentile_cont(0.95) WITHIN GROUP (ORDER BY queue_time) as perc_95,
			percentile_cont(0.99) WITHIN GROUP (ORDER BY queue_time) as perc_99,
			max(queue_time)
		from temp_queue_times
	EOF
}

query_queue() { ## query queue: Brief overview of currently running jobs
	handle_help "$@" <<-EOF
		    $ gxadmin query queue
		                                tool_id                                |  state  | count
		    -------------------------------------------------------------------+---------+-------
		     toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0      | queued  |     9
		     toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0     | running |     7
		     toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                | queued  |     6
		     ebi_sra_main                                                      | running |     6
		     toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.8.3            | queued  |     5
		     toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.3.4.2      | running |     5
		     toolshed.g2.bx.psu.edu/repos/nml/spades/spades/3.11.1+galaxy1     | queued  |     4
		     toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0 | running |     2
		     toolshed.g2.bx.psu.edu/repos/nml/metaspades/metaspades/3.9.0      | running |     2
		     upload1                                                           | running |     2
	EOF

	fields="count=2"
	tags="tool_id=0;state=1"

	read -r -d '' QUERY <<-EOF
			SELECT tool_id, state, count(*)
			FROM job
			WHERE state in ('queued', 'running')
			GROUP BY tool_id, state
			ORDER BY count desc
	EOF
}

query_queue_overview() { ## query queue-overview: View used mostly for monitoring
	handle_help "$@" <<-EOF
		Primarily for monitoring of queue. Optimally used with 'iquery' and passed to Telegraf.

		    $ gxadmin iquery queue-overview
		    queue-overview,tool_id=test_history_sanitization,tool_version=0.0.1,state=running,handler=main.web.1,destination_id=condor,job_runner_name=condor count=1

	EOF

	fields="count=6"
	tags="tool_id=0;tool_version=1;destination_id=2;handler=3;state=4;job_runner_name=5"

	read -r -d '' QUERY <<-EOF
		SELECT
			tool_id, tool_version, destination_id, handler, state, job_runner_name, count(*) as count
		FROM job
		WHERE
			state = 'running' or state = 'queued'
		GROUP BY
			tool_id, tool_version, destination_id, handler, state, job_runner_name
	EOF
}

query_queue_detail() { ## query queue-detail [--all]: Detailed overview of running and queued jobs
	handle_help "$@" <<-EOF
		    $ gxadmin query queue-detail
		      state  |   id    |  extid  |                                 tool_id                                   |      username       | time_since_creation
		    ---------+---------+---------+---------------------------------------------------------------------------+---------------------+---------------------
		     running | 4360629 | 229333  | toolshed.g2.bx.psu.edu/repos/bgruening/infernal/infernal_cmsearch/1.1.2.0 |                     | 5 days 11:00:00
		     running | 4362676 | 230237  | toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0         |                     | 4 days 18:00:00
		     running | 4364499 | 231055  | toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0         |                     | 4 days 05:00:00
		     running | 4366604 | 5183013 | toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0             |                     | 3 days 20:00:00
		     running | 4366605 | 5183016 | toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0             |                     | 3 days 20:00:00
		     queued  | 4350274 | 225743  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              |                     | 9 days 05:00:00
		     queued  | 4353435 | 227038  | toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.8.3                    |                     | 8 days 08:00:00
		     queued  | 4361914 | 229712  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              |                     | 5 days -01:00:00
		     queued  | 4361812 | 229696  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              |                     | 5 days -01:00:00
		     queued  | 4361939 | 229728  | toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                        |                     | 4 days 21:00:00
		     queued  | 4361941 | 229731  | toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                        |                     | 4 days 21:00:00
	EOF

	d=""
	if [[ $1 == "--all" ]]; then
		d=", 'new'"
	fi

		read -r -d '' QUERY <<-EOF
			SELECT job.state, job.id, job.job_runner_external_id as extid, job.tool_id, galaxy_user.username, date_trunc('hour', (now() - job.create_time - '2 hours'::interval)) as time_since_creation
			FROM job, galaxy_user
			WHERE state in ('running', 'queued'$d) and job.user_id = galaxy_user.id
			ORDER BY state desc, time_since_creation desc
	EOF
}

query_runtime_per_user() { ## query runtime-per-user <email>: computation time of user (by email)
	handle_help "$@" <<-EOF
		    $ gxadmin query runtime-per-user hxr@informatik.uni-freiburg.de
		       sum
		    ----------
		     14:07:39
	EOF

	assert_count $# 1 "Missing user"

	read -r -d '' QUERY <<-EOF
			SELECT sum((metric_value || ' second')::interval)
			FROM job_metric_numeric
			WHERE job_id in (
				SELECT id
				FROM job
				WHERE user_id in (
					SELECT id
					FROM galaxy_user
					where email = '$1'
				)
			) AND metric_name = 'runtime_seconds'
	EOF
}

query_jobs_per_user() { ## query jobs-per-user <email>: Number of jobs run by a specific user
	handle_help "$@" <<-EOF
		    $ gxadmin query jobs-per-user hxr@informatik.uni-freiburg.de
		     count
		    -------
		      1460
	EOF

	assert_count $# 1 "Missing user"
	read -r -d '' QUERY <<-EOF
			SELECT count(id)
			FROM job
			WHERE user_id in (
				SELECT id
				FROM galaxy_user
				WHERE email = '$1'
	EOF
}

query_recent_jobs() { ## query recent-jobs <hours>: Jobs run in the past <hours> (in any state)
	handle_help "$@" <<-EOF
		Note that your database may have a different TZ than your querying. This is probably a misconfiguration on our end, please let me know how to fix it. Just add your offset to UTC to your query.

		    $ gxadmin query recent-jobs 2.1
		       id    |     date_trunc      |      tool_id          | state |    username
		    ---------+---------------------+-----------------------+-------+-----------------
		     4383997 | 2018-10-05 16:07:00 | Filter1               | ok    |
		     4383994 | 2018-10-05 16:04:00 | echo_main_condor      | ok    |
		     4383993 | 2018-10-05 16:04:00 | echo_main_drmaa       | error |
		     4383992 | 2018-10-05 16:04:00 | echo_main_handler11   | ok    |
		     4383983 | 2018-10-05 16:04:00 | echo_main_handler2    | ok    |
		     4383982 | 2018-10-05 16:04:00 | echo_main_handler1    | ok    |
		     4383981 | 2018-10-05 16:04:00 | echo_main_handler0    | ok    |
	EOF

	assert_count $# 1 "Missing hours"

	read -r -d '' QUERY <<-EOF
			SELECT
				job.id, date_trunc('minute', job.create_time), job.tool_id, job.state, galaxy_user.username
			FROM job, galaxy_user
			WHERE job.create_time > (now() - '$1 hours'::interval) AND job.user_id = galaxy_user.id
			ORDER BY id desc
	EOF
}

query_training() { ## query training [--all]: List known trainings
	handle_help "$@" <<-EOF
		This module is specific to EU's implementation of Training Infrastructure as a Service. But this specifically just checks for all groups with the name prefix `training-`

		    $ gxadmin query training
		           name       |  created
		    ------------------+------------
		     hts2018          | 2018-09-19
	EOF

	d1=""
	d2="AND deleted = false"
	if [[ $1 == "--all" ]]; then
		d1=", deleted"
		d2=""
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			substring(name from 10) as name,
			date_trunc('day', create_time)::date as created
			$d1
		FROM galaxy_group
		WHERE name like 'training-%' $d2
		ORDER BY create_time DESC
	EOF
}

query_training_members() { ## query training-members <tr_id>: List users in a specific training
	handle_help "$@" <<-EOF
		    $ gxadmin query training-members hts2018
		          username      |       joined
		    --------------------+---------------------
		     helena-rasche      | 2018-09-21 21:42:01
	EOF

	assert_count $# 1 "Missing Training ID"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	read -r -d '' QUERY <<-EOF
			SELECT
				galaxy_user.username,
				date_trunc('second', user_group_association.create_time) as joined
			FROM galaxy_user, user_group_association, galaxy_group
			WHERE galaxy_group.name = 'training-$ww'
				AND galaxy_group.id = user_group_association.group_id
				AND user_group_association.user_id = galaxy_user.id
	EOF
}

query_training_memberof() { ## query training-memberof <username>: List trainings that a user is part of
	handle_help "$@" <<-EOF
	EOF

	assert_count $# 1 "Missing Training ID"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	read -r -d '' QUERY <<-EOF
			SELECT
                galaxy_group.name
			FROM
                galaxy_user, user_group_association, galaxy_group
			WHERE   galaxy_group.id = user_group_association.group_id
				AND user_group_association.user_id = galaxy_user.id
                AND galaxy_user.username='$ww'
	EOF
}

query_training_remove_member() { ## query training-remove-member <training> <username> [YESDOIT]: Remove a user from a training
	handle_help "$@" <<-EOF
	EOF

	assert_count_ge $# 2 "Missing parameters"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	if (( $# == 3 )) && [[ "$3" == "YESDOIT" ]]; then
		results="$(query_tsv "$qstr")"
		uga_id=$(echo "$results" | awk -F'\t' '{print $1}')
		if (( uga_id > -1 )); then
			qstr="delete from user_group_association where id = $uga_id"
		fi
		echo $qstr
	else
		read -r -d '' QUERY <<-EOF
			SELECT
				user_group_association.id,
				galaxy_user.username,
				galaxy_group.name
			FROM
				user_group_association
			LEFT JOIN galaxy_user ON user_group_association.user_id = galaxy_user.id
			LEFT JOIN galaxy_group ON galaxy_group.id = user_group_association.group_id
			WHERE
				galaxy_group.name = 'training-$ww'
				AND galaxy_user.username = '$2'
		EOF
	fi
}

query_training_queue() { ## query training-queue <training_id>: Jobs currently being run by people in a given training
	handle_help "$@" <<-EOF
		Finds all jobs by people in that queue (including things they are executing that are not part of a training)

		    $ gxadmin query training-queue hts2018
		     state  |   id    | extid  | tool_id |   username    |       created
		    --------+---------+--------+---------+---------------+---------------------
		     queued | 4350274 | 225743 | upload1 |               | 2018-09-26 10:00:00
	EOF

	assert_count $# 1 "Missing Training ID"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	read -r -d '' QUERY <<-EOF
			SELECT
				job.state,
				job.id,
				job.job_runner_external_id AS extid,
				job.tool_id,
				galaxy_user.username,
				date_trunc('hour', job.create_time) AS created
			FROM
				job, galaxy_user
			WHERE
				job.user_id = galaxy_user.id
				AND job.create_time > (now() - '3 hours'::interval)
				AND galaxy_user.id
					IN (
							SELECT
								galaxy_user.id
							FROM
								galaxy_user, user_group_association, galaxy_group
							WHERE
								galaxy_group.name = 'training-$ww'
								AND galaxy_group.id = user_group_association.group_id
								AND user_group_association.user_id = galaxy_user.id
						)
			ORDER BY
				job.create_time ASC
	EOF
}

query_disk_usage() { ## query disk-usage: Disk usage per object store.
	handle_help "$@" <<-EOF
		TODO: implement flag for --nice numbers

		     object_store_id |      sum
		    -----------------+----------------
		     files8          | 88109503720067
		     files6          | 64083627169725
		     files9          | 53690953947700
		     files7          | 30657241908566
		     files1          | 30633153627407
		     files2          | 22117477087642
		     files3          | 21571951600351
		     files4          | 13969690603365
		                     |  6943415154832
		     secondary       |   594632335718
		    (10 rows)
	EOF

	fields="count=1"
	tags="object_store_id=0"

	read -r -d '' QUERY <<-EOF
			SELECT
				object_store_id, sum(total_size)
			FROM dataset
			WHERE NOT purged
			GROUP BY object_store_id
			ORDER BY sum DESC
	EOF
}


query_user_details() { ## query user-details:
	handle_help "$@" <<-EOF
	EOF

	# Metada
	read -r -d '' qstr <<-EOF
		SELECT
			username, id, email, create_time, external, deleted, purged, active, pg_size_pretty(disk_usage)
		FROM
			galaxy_user
		WHERE
			username = '$1' or id = CAST(REGEXP_REPLACE(COALESCE('$1','0'), '[^0-9]+', '0', 'g') AS INTEGER)
	EOF
	results=$(query_tsv "$qstr")
	user_id=$(echo "$results" | awk '{print $2}')

	# Groups
	read -r -d '' qstr <<-EOF
		select string_agg(galaxy_group.name, ', ') from user_group_association, galaxy_group where user_id = $user_id and user_group_association.group_id = galaxy_group.id
	EOF
	group_membership=$(query_tsv "$qstr")

	read -r -d '' qstr <<-EOF
		select string_agg(role.name, ', ') from user_role_association, role where user_id = $user_id and user_role_association.role_id = role.id and role.type not in ('private', 'sharing')
	EOF
	role_membership=$(query_tsv "$qstr")

	# Recent jobs
	read -r -d '' qstr <<-EOF
		SELECT
			tool_id, state, create_time, exit_code, metric_value::text::interval
		FROM
			job, job_metric_numeric
		WHERE
			job.user_id = $user_id and job.id = job_metric_numeric.job_id and metric_name = 'runtime_seconds' order by job.id desc limit 10
	EOF
	recent_jobs=$(query_tsv "$qstr")
	recent_jobs2=$(printf "Tool ID\tStatus\tCreated\tExit Code\tRuntime\n----\t----\t----\t---\t----\n%s" "$recent_jobs" | sed 's/\t/\t | \t/g' | column -t -s'	')

	read -r -d '' template <<EOF
# Galaxy User $user_id

  Property | Value
---------- | -----
        ID | %s (id=%s) %s
   Created | %s %s
Properties | ext=%s deleted=%s purged=%s active=%s
Disk Usage | %s %s

## Groups/Roles

Groups: %s
Roles: %s

## Recent Jobs

%s
\n
EOF
	printf "$template" $results "$group_membership" "$role_membership" "$recent_jobs2"
}

query_users_count() { ## query users-count: Shows sums of active/external/deleted/purged accounts
	handle_help "$@" <<-EOF
		     active | external | deleted | purged | count
		    --------+----------+---------+--------+-------
		     f      | f        | f       | f      |   182
		     t      | f        | t       | t      |     2
		     t      | f        | t       | f      |     6
		     t      | f        | f       | f      |  2350
		     f      | f        | t       | t      |    36
	EOF

	fields="count=4"
	tags="active=0;external=1;deleted=2;purged=3"

	read -r -d '' QUERY <<-EOF
			SELECT
				active, external, deleted, purged, count(*) as count
			FROM
				galaxy_user
			GROUP BY
				active, external, deleted, purged
	EOF
}

query_tool_last_used_date() { ## query tool-last-used-date: When was the most recent invocation of every tool
	handle_help "@" <<-EOF
		It is not truly every tool, there is no easy way to find the tools which have never been run.
	EOF

	read -r -d '' QUERY <<-EOF
		select max(date_trunc('month', create_time)), tool_id from job group by tool_id order by max desc
	EOF
}

query_users_total() { ## query users-total: Total number of Galaxy users (incl deleted, purged, inactive)
	handle_help "$@" <<-EOF
	EOF

	fields="count=0"
	tags=""

	read -r -d '' QUERY <<-EOF
			SELECT count(*) FROM galaxy_user
	EOF
}

query_groups_list() { ## query groups-list: List all groups known to Galaxy
	handle_help "$@" <<-EOF
	EOF

	fields="count=1"
	tags="group_name=0"

	read -r -d '' QUERY <<-EOF
			SELECT
				galaxy_group.name, count(*)
			FROM
				galaxy_group, user_group_association
			WHERE
				user_group_association.group_id = galaxy_group.id
			GROUP BY name
	EOF
}

query_collection_usage() { ## query collection-usage: Information about how many collections of various types are used
	handle_help "$@" <<-EOF
	EOF

	fields="count=1"
	tags="group_name=0"

	read -r -d '' QUERY <<-EOF
		SELECT
			dc.collection_type, count(*)
		FROM
			history_dataset_collection_association as hdca
		INNER JOIN
			dataset_collection as dc
			ON hdca.collection_id = dc.id
		GROUP BY
			dc.collection_type;
	EOF
}

query_ts_repos() { ## query ts-repos: Counts of toolshed repositories by toolshed and owner.
	handle_help "$@" <<-EOF
	EOF

	fields="count=2"
	tags="tool_shed=0;owner=1"

	read -r -d '' QUERY <<-EOF
			SELECT
				tool_shed, owner, count(*)
			FROM
				tool_shed_repository
			GROUP BY
				tool_shed, owner
	EOF
}

query_active_users() { ## query active-users [weeks]: Count of users who ran jobs in past 1 week (default = 1)
	handle_help "$@" <<-EOF
		Unique users who ran jobs in past week:

		    $ gxadmin query active-users
		     count
		    -------
		       220
		    (1 row)

		Or the monthly-active-users:

		    $ gxadmin query active-users 4
		     count
		    -------
		       555
		    (1 row)
	EOF

	weeks=1
	if (( $# > 0 )); then
		weeks=$1
	fi

	fields="count=1"
	tags="weeks=0"


	read -r -d '' QUERY <<-EOF
		SELECT
			$weeks as weeks,
			count(distinct user_id)
		FROM
			job
		WHERE
			job.create_time > (now() - '$weeks weeks'::interval)
	EOF
}

query_tool_metrics() { ## query tool-metrics <tool_id> <metric_id> [--like]: See values of a specific metric
	handle_help "$@" <<-EOF
		A good way to use this is to fetch the memory usage of a tool and then
		do some aggregations. The following requires [data_hacks](https://github.com/bitly/data_hacks)

		    $ gxadmin tsvquery tool-metrics %rgrnastar/rna_star% memory.max_usage_in_bytes --like | \\
		        awk '{print $1 / 1024 / 1024 / 1024}' | \\
		        histogram.py --percentage
		    # NumSamples = 441; Min = 2.83; Max = 105.88
		    # Mean = 45.735302; Variance = 422.952289; SD = 20.565804; Median 51.090900
		    # each ∎ represents a count of 1
		        2.8277 -    13.1324 [    15]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (3.40%)
		       13.1324 -    23.4372 [    78]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (17.69%)
		       23.4372 -    33.7419 [    47]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (10.66%)
		       33.7419 -    44.0466 [    31]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (7.03%)
		       44.0466 -    54.3514 [    98]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (22.22%)
		       54.3514 -    64.6561 [   102]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (23.13%)
		       64.6561 -    74.9608 [    55]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (12.47%)
		       74.9608 -    85.2655 [    11]: ∎∎∎∎∎∎∎∎∎∎∎ (2.49%)
		       85.2655 -    95.5703 [     3]: ∎∎∎ (0.68%)
		       95.5703 -   105.8750 [     1]: ∎ (0.23%)
	EOF

	assert_count_ge $# 1 "Missing Tool ID"
	assert_count_ge $# 2 "Missing Metric ID (hint: tool-available-metrics)"

	tool_subquery="SELECT id FROM job WHERE tool_id = '$1'"
	if [[ "$3" == "--like" ]]; then
		tool_subquery="SELECT id FROM job WHERE tool_id like '$1'"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			metric_value
		FROM job_metric_numeric
		WHERE
			metric_name = '$2'
			and
			job_id in (
				$tool_subquery
			)
	EOF
}

query_tool_available_metrics() { ## query tool-available-metrics <tool_id>: list all available metrics for a given tool
	handle_help "$@" <<-EOF
	EOF

	assert_count $# 1 "Missing Tool ID"

	read -r -d '' QUERY <<-EOF
		SELECT
			distinct metric_name
		FROM job_metric_numeric
		WHERE job_id in (
			SELECT id FROM job WHERE tool_id = '$1'
		)
	EOF
}

query_month_data(){ ## query monthly data-created [year]: Number of active users per month, running jobs
	handle_help "$@" <<-EOF
	EOF

	if (( $# > 0 )); then
		where="WHERE date_trunc('year', dataset.create_time) = '$1-01-01'::date"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			pg_size_pretty(sum(total_size)), date_trunc('month', dataset.create_time) AS month
		FROM
			dataset
		$where
		GROUP BY
			month
		ORDER BY
			month DESC;
	EOF
}

query_month_users(){ ## query monthly active-users [year]: Number of active users per month, running jobs
	handle_help "$@" <<-EOF
	EOF

	if (( $# > 0 )); then
		where="WHERE date_trunc('year', job.create_time) = '$1-01-01'::date"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			count(distinct user_id) as unique_users,
			date_trunc('month', job.create_time) as month
		FROM job
		$where
		GROUP BY month
		ORDER BY month DESC
	EOF
}

query_month_jobs(){ ## query monthly jobs [year]: Number of jobs run each month
	handle_help "$@" <<-EOF
	EOF

	if (( $# > 0 )); then
		where="WHERE date_trunc('year', job.create_time) = '$1-01-01'::date"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			date_trunc('month', job.create_time) AS month,
			count(*)
		FROM
			job
		$where
		GROUP BY
			month
		ORDER BY
			month DESC
	EOF

}

query_job_history() { ## query job-history <id>: Job state history for a specific job
	handle_help "$@" <<-EOF
		    $ gxadmin query job-history 4384025
		            time         |  state
		    ---------------------+---------
		     2018-10-05 16:20:13 | ok
		     2018-10-05 16:19:57 | running
		     2018-10-05 16:19:55 | queued
		     2018-10-05 16:19:54 | new
		    (4 rows)
	EOF

	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				date_trunc('second', create_time) as time,
				state
			FROM job_state_history
			WHERE job_id = $1
	EOF
}

query_job_inputs() { ## query job-inputs <id>: Input datasets to a specific job
	handle_help "$@" <<-EOF
	EOF
	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				hda.id AS hda_id,
				hda.state AS hda_state,
				hda.deleted AS hda_deleted,
				hda.purged AS hda_purged,
				d.id AS d_id,
				d.state AS d_state,
				d.deleted AS d_deleted,
				d.purged AS d_purged,
				d.object_store_id AS object_store_id
			FROM job j
				JOIN job_to_input_dataset jtid
					ON j.id = jtid.job_id
				JOIN history_dataset_association hda
					ON hda.id = jtid.dataset_id
				JOIN dataset d
					ON hda.dataset_id = d.id
			WHERE j.id = $1
	EOF
}

query_job_outputs() { ## query job-outputs <id>: Output datasets from a specific job
	handle_help "$@" <<-EOF
	EOF

	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				hda.id AS hda_id,
				hda.state AS hda_state,
				hda.deleted AS hda_deleted,
				hda.purged AS hda_purged,
				d.id AS d_id,
				d.state AS d_state,
				d.deleted AS d_deleted,
				d.purged AS d_purged,
				d.object_store_id AS object_store_id
			FROM job j
				JOIN job_to_output_dataset jtod
					ON j.id = jtod.job_id
				JOIN history_dataset_association hda
					ON hda.id = jtod.dataset_id
				JOIN dataset d
					ON hda.dataset_id = d.id
			WHERE j.id = $1
		"
	EOF
}

query_old_histories(){ ## query old-histories <weeks>: Lists histories that haven't been updated (used) for <weeks>
	handle_help "$@" <<-EOF
		Histories and their users who haven't been updated for a year. Default number of weeks is 15.

		$gxadmin query old-histories 52
		  id   |        update_time         | user_id |  email  |       name         | published | deleted | purged | hid_counter
		-------+----------------------------+---------+---------+--------------------+-----------+---------+--------+-------------
		 39903 | 2017-06-13 12:35:07.174749 |     834 | xxx@xxx | Unnamed history    | f         | f       | f      |          23
		  1674 | 2017-06-13 14:08:30.017574 |       9 | xxx@xxx | SAHA project       | f         | f       | f      |          47
		 40088 | 2017-06-15 04:10:48.879122 |     986 | xxx@xxx | Unnamed history    | f         | f       | f      |           3
		 39023 | 2017-06-15 09:33:12.007002 |     849 | xxx@xxx | prac 4 new final   | f         | f       | f      |         297
		 35437 | 2017-06-16 04:41:13.15785  |     731 | xxx@xxx | Unnamed history    | f         | f       | f      |          98
		 40123 | 2017-06-16 13:43:24.948344 |     987 | xxx@xxx | Unnamed history    | f         | f       | f      |          22
		 40050 | 2017-06-19 00:46:29.032462 |     193 | xxx@xxx | Telmatactis        | f         | f       | f      |          74
		 12212 | 2017-06-20 14:41:03.762881 |     169 | xxx@xxx | Unnamed history    | f         | f       | f      |          24
		 39523 | 2017-06-21 01:34:52.226653 |       9 | xxx@xxx | OSCC Cell Lines    | f         | f       | f      |         139
	EOF

	weeks=15
	read -r -d '' QUERY <<-EOF
		SELECT
			h.id,
			h.update_time,
			h.user_id,
			u.email,
			h.name,
			h.published,
			h.deleted,
			h.purged,
			h.hid_counter
		FROM
			history h,
			galaxy_user u
		WHERE
			h.update_time < (now() - '$weeks weeks'::interval) AND
			h.user_id = u.id AND
			h.deleted = FALSE AND
			h.published = FALSE
		ORDER BY
			h.update_time
	EOF
}

update() { ## update: Update the script
	handle_help "$@" <<-EOF
	EOF

	tmp=$(mktemp);
	curl https://raw.githubusercontent.com/usegalaxy-eu/gxadmin/master/gxadmin > $tmp;
	chmod ugo+x $tmp;
	chmod ugo+r $tmp;
	mv $tmp $0;
	exit 0
}

version() {
	echo 11
}


cmdlist() {
	IFS=$'\n'
	# TOC
	echo "Command | Description"
	echo "------- | -----------"
	for command in $(grep -o ' ## .*' $0 | grep -v grep | sort | sed 's/^ ## //g'); do
		cmd_part="$(echo $command | sed 's/:.*//g;s/\s*<.*//g;s/\s*\[.*//')"
		desc_part="$(echo $command | sed 's/^[^:]*:\s*//g')"
		key_part="$(echo $cmd_part | sed 's/ /-/g')"
		echo "[${cmd_part}](#${key_part}) | $desc_part"
	done
	echo

	# Now for sections
	for command in $(grep -o ' ## .*' $0 | grep -v grep | sort | sed 's/^ ## //g'); do
		cmd_part="$(echo $command | sed 's/:.*//g;s/\s*<.*//g;s/\s*\[.*//')"
		echo
		echo "### $cmd_part"
		echo
		bash -c "$0 $cmd_part --help"
	done

}


obtain_query() {
	query_name="$1"; shift

	case "$query_name" in
		active-users           ) query_active_users           "$@" ;;
		collection-usage       ) query_collection_usage       "$@" ;;
		datasets-created-daily ) query_datasets_created_daily "$@" ;;
		disk-usage             ) query_disk_usage             "$@" ;;
		groups-list            ) query_groups_list            "$@" ;;
		job-history            ) query_job_history            "$@" ;;
		job-info               ) query_job_info               "$@" ;;
		job-inputs             ) query_job_inputs             "$@" ;;
		job-outputs            ) query_job_outputs            "$@" ;;
		jobs-per-user          ) query_jobs_per_user          "$@" ;;
		largest-collection     ) query_largest_collection     "$@" ;;
		latest-users           ) query_latest_users           "$@" ;;
		monthly-data           ) query_month_data             "$@" ;;
		monthly-jobs           ) query_month_jobs             "$@" ;;
		monthly-users          ) query_month_users            "$@" ;;
		old-histories          ) query_old_histories          "$@" ;;
		queue                  ) query_queue                  "$@" ;;
		queue-detail           ) query_queue_detail           "$@" ;;
		queue-overview         ) query_queue_overview         "$@" ;;
		queue-time             ) query_queue_time             "$@" ;;
		recent-jobs            ) query_recent_jobs            "$@" ;;
		runtime-per-user       ) query_runtime_per_user       "$@" ;;
		tool-available-metrics ) query_tool_available_metrics "$@" ;;
		tool-last-used-date    ) query_tool_last_used_date    "$@" ;;
		tool-metrics           ) query_tool_metrics           "$@" ;;
		tool-usage             ) query_tool_usage             "$@" ;;
		training               ) query_training               "$@" ;;
		training-memberof      ) query_training_memberof      "$@" ;;
		training-members       ) query_training_members       "$@" ;;
		training-queue         ) query_training_queue         "$@" ;;
		training-remove-member ) query_training_remove_member "$@" ;;
		ts-repos               ) query_ts_repos               "$@" ;;
		user-details           ) query_user_details           "$@" ;;
		user-info              ) query_user_details           "$@" ;;
		users-count            ) query_users_count            "$@" ;;
		users-total            ) query_users_total            "$@" ;;

		# default
		* ) export QUERY="ERROR" ;;
	esac
}

query() {
	# do the thing zhu li
	query_type="$1"; shift
	subfunc="$1"; shift

	# We do not run this in a subshell because we need to "return" multiple things.
	obtain_query $subfunc "$@"

	# TODO(hxr)
	#ec=$?
	#if (( ec > 0 )); then
		#echo "$db_query"
		#exit 0
	#fi
	if [[ "$QUERY" == "ERROR" ]]; then
		usage
	fi

	# Run the queries
	case "$query_type" in
		tsvquery )  query_tsv "$QUERY";;
		csvquery )  query_csv "$QUERY";;
		query    )  query_tbl "$QUERY";;
		iquery   )  query_influx "$QUERY" "$subfunc" "$fields" "$tags";;
		# default
		*        )  usage "Error";;
	esac
}


hexdecode() { ## filter hexdecode: Decodes any hex blobs from postgres outputs
	handle_help "$@" <<-EOF
		This automatically replaces any hex strings (\\x[a-f0-9]+) with their decoded versions. This can allow you to query galaxy metadata, decode it, and start processing it with JQ. Just pipe your query to it and it will replace it wherever it is found.

		    [galaxy@sn04 ~]$ psql -c  'select metadata from history_dataset_association limit 10;'
		                                 metadata
		    ------------------------------------------------------------------------------------------------------------------
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a20333239312c202264626b6579223a205b223f225d2c202273657175656e636573223a20317d
		     \\x7b22646174615f6c696e6573223a20312c202264626b6579223a205b223f225d7d
		     \\x7b22646174615f6c696e6573223a20312c202264626b6579223a205b223f225d7d
		     \\x7b22646174615f6c696e6573223a20312c202264626b6579223a205b223f225d7d
		     \\x7b22646174615f6c696e6573223a20312c202264626b6579223a205b223f225d7d
		    (10 rows)

		    [galaxy@sn04 ~]$ psql -c  'select metadata from history_dataset_association limit 10;'  | gxadmin filter hexdecode
		                                 metadata
		    ------------------------------------------------------------------------------------------------------------------
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": 3291, "dbkey": ["?"], "sequences": 1}
		     {"data_lines": 1, "dbkey": ["?"]}
		     {"data_lines": 1, "dbkey": ["?"]}
		     {"data_lines": 1, "dbkey": ["?"]}
		     {"data_lines": 1, "dbkey": ["?"]}
		    (10 rows)

		Or to query for the dbkeys uesd by datasets:

		    [galaxy@sn04 ~]$ psql -c  'copy (select metadata from history_dataset_association limit 1000) to stdout' | \\
		        gxadmin filter hexdecode | \\
		        jq -r '.dbkey[0]' 2>/dev/null | sort | uniq -c | sort -nr
		        768 ?
		        118 danRer7
		         18 hg19
		         17 mm10
		         13 mm9
		          4 dm3
		          1 TAIR10
		          1 hg38
		          1 ce10


	EOF

	cat /dev/stdin | python -c "$hexdecodelines"
}


filter() {
	subfunc="$1"; shift
	case "$subfunc" in
		hexdecode  ) hexdecode "$@" ;;
	esac
}

uwsgi() {
	subfunc="$1"; shift
	case "$subfunc" in
		stats_influx  ) uwsgi_stats_influx "$@";;
	esac
}

uwsgi_stats_influx(){ ## uwsgi stats_influx <addr>: InfluxDB formatted output for the current stats
	handle_help "$@" <<-EOF
		Contact a specific uWSGI stats address (requires uwsgi binary on path)
		and requests the current stats + formats them for InfluxDB. For some
		reason it has trouble with localhost vs IP address, so recommend that
		you use IP.

		    $ gxadmin uwsgi stats_influx 127.0.0.1:9191
		    uwsgi.locks,addr=127.0.0.1:9191,group=user_0 count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=signal count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=filemon count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=timer count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=rbtimer count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=cron count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=thunder count=2006859
		    uwsgi.locks,addr=127.0.0.1:9191,group=rpc count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=snmp count=0
		    uwsgi.general,addr=127.0.0.1:9191 listen_queue=0,listen_queue_errors=0,load=0,signal_queue=0
		    uwsgi.sockets,addr=127.0.0.1:9191,name=127.0.0.1:4001,proto=uwsgi queue=0,max_queue=100,shared=0,can_offload=0
		    uwsgi.worker,addr=127.0.0.1:9191,id=1 accepting=1,requests=65312,exceptions=526,harakiri_count=26,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=17433008661,respawn_count=27,tx=15850829410,avg_rt=71724
		    uwsgi.worker,addr=127.0.0.1:9191,id=2 accepting=1,requests=67495,exceptions=472,harakiri_count=51,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=15467746010,respawn_count=52,tx=15830867066,avg_rt=65380
		    uwsgi.worker,addr=127.0.0.1:9191,id=3 accepting=1,requests=67270,exceptions=520,harakiri_count=35,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=14162158015,respawn_count=36,tx=15799661545,avg_rt=73366
		    uwsgi.worker,addr=127.0.0.1:9191,id=4 accepting=1,requests=66434,exceptions=540,harakiri_count=34,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=15740205807,respawn_count=35,tx=16231969649,avg_rt=75468
		    uwsgi.worker,addr=127.0.0.1:9191,id=5 accepting=1,requests=67021,exceptions=534,harakiri_count=38,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=14573155758,respawn_count=39,tx=16517287963,avg_rt=140855
		    uwsgi.worker,addr=127.0.0.1:9191,id=6 accepting=1,requests=66810,exceptions=483,harakiri_count=24,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=19107513635,respawn_count=25,tx=15945313469,avg_rt=64032
		    uwsgi.worker,addr=127.0.0.1:9191,id=7 accepting=1,requests=66544,exceptions=460,harakiri_count=35,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=14240478391,respawn_count=36,tx=15499531841,avg_rt=114981
		    uwsgi.worker,addr=127.0.0.1:9191,id=8 accepting=1,requests=67577,exceptions=517,harakiri_count=35,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=14767971195,respawn_count=36,tx=15780639229,avg_rt=201275

		For multiple zerglings you can run this for each and just 2>/dev/null

		    PATH=/opt/galaxy/venv/bin:/sbin:/bin:/usr/sbin:/usr/bin gxadmin uwsgi stats_influx 127.0.0.1:9190 2>/dev/null
		    PATH=/opt/galaxy/venv/bin:/sbin:/bin:/usr/sbin:/usr/bin gxadmin uwsgi stats_influx 127.0.0.1:9191 2>/dev/null
		    exit 0

		And it will fetch only data for responding uwsgis.
	EOF
	address="$1"; shift

	# fetch data
	uwsgi=$(which uwsgi)
	data="$($uwsgi --connect-and-read $address 2>&1)"

	echo "$data" | \
		jq -r '.locks[] | to_entries[] |  "uwsgi.locks,addr='$address',group=\(.key) count=\(.value)"' | \
		sed 's/group=user 0/group=user_0/g'

	echo "$data" | \
		jq -r '. | "uwsgi.general,addr='$address' listen_queue=\(.listen_queue),listen_queue_errors=\(.listen_queue_errors),load=\(.load),signal_queue=\(.signal_queue)"'

	echo "$data" | \
		jq -r '.sockets[] | "uwsgi.sockets,addr='$address',name=\(.name),proto=\(.proto) queue=\(.queue),max_queue=\(.max_queue),shared=\(.shared),can_offload=\(.can_offload)"'

	echo "$data" | \
		jq -r '.workers[] | "uwsgi.worker,addr='$address',id=\(.id) accepting=\(.accepting),requests=\(.requests),exceptions=\(.exceptions),harakiri_count=\(.harakiri_count),signals=\(.signals),signal_queue=\(.signal_queue),status=\"\(.status)\",rss=\(.rss),vsz=\(.vsz),running_time=\(.running_time),respawn_count=\(.respawn_count),tx=\(.tx),avg_rt=\(.avg_rt)"' | \
		sed 's/"busy"/1/g;s/"idle"/0/g;'
}




mode="$1"; shift

case "$mode" in
	validate                       ) func_validate     "$@" ;;
	cleanup                        ) cleanup           "$@" ;;
	zerg                           ) zerg              "$@" ;;
	handler                        ) handler           "$@" ;;
	migrate-tool-install-to-sqlite ) migrate_to_sqlite "$@" ;;
	update                         ) update            "$@" ;;
	uwsgi                          ) uwsgi             "$@" ;;
        filter                         ) filter            "$@" ;;
        *query                         ) query "$mode"     "$@" ;;

	# Generate for readme:
	cmdlist   ) cmdlist ;;
	# version commands
	version   ) version ;;
	-v        ) version ;;
	--version ) version ;;
	# help
	help      ) usage safe ;;
	-h        ) usage safe ;;
	--help    ) usage safe ;;
	# anything else
	*         ) usage "Unknown command";;
esac
