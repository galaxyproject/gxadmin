#!/bin/bash
# gxadmin: Galaxy administration swiss army knife
# license: gplv3
#
# This is the result of admins across the universe working together :)
#
# Thanks to everyone for their contributions
# https://github.com/usegalaxy-eu/gxadmin/graphs/contributors
version() {
	echo 12
}
GXADMIN_SITE_SPECIFIC=${GXADMIN_SITE_SPECIFIC:-~/.config/gxadmin-local.sh}

hexdecodelines=$(cat <<EOF
import sys
import re
import binascii
query = re.compile(r'^(.*)\\\\x([0-9a-f]+)(.*)$')
for line in sys.stdin:
	out = '%s' % line.replace('\\\\\\\\', '\\\\')
	while True:
		m = query.match(out)
		if m:
			out = m.groups()[0]
			out += binascii.unhexlify(m.groups()[1])
			out += m.groups()[2] + '\\n'
		else:
			break
	sys.stdout.write(out)
EOF
)

identicon_script=$(cat <<EOF
import sys
import hashlib

def colored(fg, bg, value):
    return '\x1b[38;5;{0}m\x1b[48;5;{1}m{2}\x1b[0m'.format(fg, bg, value)

class Iden:
    def build(self, h):
        pixels = [[0 for _ in range(5)] for _ in range(5)]
        for i in range(0, 5):
            for x in range(0, 5):
                pixels[x][i] = self.showPixel(i, x, h)
        return pixels

    def showPixel(self, x, y, h):
        m = 6 + abs(2-x) * 5 + y
        return int(h[m:m+1], 16) % 2 == 0

    def getIcon(self, num, hash_string):
        for x in self.build(hash_string):
            yield ''.join([
                colored(num, num + 15 % 255, '██' if q else '  ')
                for q in x
            ])

data = sys.stdin.read().strip()
m = hashlib.sha256()
m.update(data.encode('UTF-8'))
hash_string = m.hexdigest()
num = int(hash_string[0:2], 16)
for line in Iden().getIcon(num, hash_string):
    print(line)
EOF
)
ifmore() {
	q="$(cat)"
	lines=$(echo "$q" | wc -l)
	# 2 more than the head command since we'll 'spend' two for the ... anyway.
	if (( lines > 8 )); then
		echo "$q" | head -n 6
		echo "    ..."
		echo "    run '$0 $1 help' for more"
	else
		echo "$q"
	fi
}

colour_word() {
	word=$1
	color=$2
	if [[ $color == "red" ]]; then
		color_idx=1
	elif [[ $color == "orange" ]]; then
		color_idx=2
	fi

	cat | sed "s|${word}|$(tput setab $color_idx)${word}$(tput sgr0)|g"
}

filter_commands() {
	if [[ $2 == $1 ]]; then
		cat | grep "^$1 " | sort -k2 | column -s: -t | sed 's/^/    /' | colour_word Deprecated orange
	else
		cat | grep "^$1 " | sort -k2 | column -s: -t | sed 's/^/    /' | ifmore "$1"
	fi
}

usage(){
	cmds="$(grep -s -h -o '{ ## .*' $0 $GXADMIN_SITE_SPECIFIC | grep -v grep | grep -v '| sed' | sort | sed 's/^{ ## //g')"

	cat <<-EOF
		gxadmin usage:

	EOF

	if (( $# == 0  )) || [[ $1 == "config" ]]; then
		cat <<-EOF
			Configuration:

			$(echo "$cmds" | filter_commands config $1)

		EOF
	fi

	if (( $# == 0  )) || [[ $1 == "filter" ]]; then
		cat <<-EOF
			Filters:

			$(echo "$cmds" | filter_commands filter $1)

		EOF
	fi

	if (( $# == 0  )) || [[ $1 == "galaxy" ]]; then
		cat <<-EOF
			Galaxy Administration:

			$(echo "$cmds" | filter_commands galaxy $1)

		EOF
	fi

	if (( $# == 0  )) || [[ $1 == "uwsgi" ]]; then
		cat <<-EOF
			uwsgi:

			$(echo "$cmds" | filter_commands uwsgi $1)

		EOF
	fi


	if (( $# == 0  )) || [[ $1 == "query" ]]; then
		cat <<-EOF
			DB Queries:
			  'query' can be exchanged with 'tsvquery' or 'csvquery' for tab- and comma-separated variants

			$(echo "$cmds" | filter_commands query $1)

		EOF
	fi

	if (( $# == 0  )) || [[ $1 == "report" ]]; then
		cat <<-EOF
			Report:
			  Consider https://github.com/ttscoff/mdless for easier reading in the terminal

			$(echo "$cmds" | filter_commands report $1)

		EOF
	fi

	if (( $# == 0  )) || [[ $1 == "mutate" ]]; then
		cat <<-EOF
			DB Mutations: (csv/tsv queries are NOT available)

			$(echo "$cmds" | filter_commands mutate $1)

		EOF
	fi

	if (( $# == 0  )) || [[ $1 == "meta" ]]; then
		cat <<-EOF
			Meta:

			$(echo "$cmds" | filter_commands meta $1)

		EOF
	fi

	if [[ -f $GXADMIN_SITE_SPECIFIC ]]; then
		if (( $# == 0  )) || [[ $1 == "local" ]]; then
			cat <<-EOF
				Local: (These can be configured in $GXADMIN_SITE_SPECIFIC)

				$(echo "$cmds" | filter_commands local $1)

			EOF
		fi
	else
		cat <<-EOF
			Local-only commands can be configured in $GXADMIN_SITE_SPECIFIC

		EOF
	fi


	cat <<-EOF
		help / -h / --help : this message. Invoke '--help' on any subcommand for help specific to that subcommand
	EOF

	exit 0;
}

handle_help() {
	if [[ ! -z "${GXADMIN_POPCON_ENABLE}" ]]; then
		if [[ "${query_name}" != "user-info" ]]; then
			echo "${mode:-mode} ${query_name:-sf}" >> ~/.gxadmin-popcon.log
		fi
	fi

	for i in "$@"; do
		if [[ $i = --help || $i = -h ]]; then

			key="${mode}"
			if [[ ! -z "${query_name}" ]]; then
				key="${key} ${query_name}"
			fi

			echo "**NAME**"
			echo
			invoke_desc=$(grep -s -h "{ ## ${key}[ :]" $0 $GXADMIN_SITE_SPECIFIC | sed "s/.*## /gxadmin /g")
			short_desc=$(echo $invoke_desc | sed 's/.*://g')
			short_parm=$(echo $invoke_desc | sed 's/:.*//g')
			echo "${key} - ${short_desc}"
			echo
			echo "**SYNOPSIS**"
			echo
			echo $short_parm
			echo
			manual="$(cat -)"
			manual_wc="$(echo $manual | wc -c)"
			if (( manual_wc > 3 )); then
				echo "**NOTES**"
				echo
				echo "$manual"
				echo
			fi
			# exit after printing the documentation!
			exit 0;
		fi
	done
}
assert_restart_lock(){
	if [ -f $HOME/.restart-lock ]; then
		echo "A restart lock exists. This means someone is probably already restarting galaxy."
		exit 3
	fi
}

assert_set_env() {
	env_var=$1
	if [[ -z "${!env_var}" ]]; then
		error Please set \$${env_var}
		exit 1
	fi
}

wait_for_url() {
	url=$1; shift;

	while [ $(curl --connect-timeout 5 --silent $url | wc -c) -eq "0" ]; do
		sleep 5;
		echo -n '.'
	done
}

assert_count() {
	if (( $1 != $2 )); then
		error "$3"
		usage
		exit 1
	fi
}

assert_count_ge() {
	if (( $1 < $2 )); then
		error "$3"
		usage
		exit 1
	fi
}
query_tbl() {
	psql <<-EOF
	$1
	EOF
}

query_tsv() {
	psql <<-EOF
	COPY ($1) to STDOUT with CSV DELIMITER E'\t'
	EOF
}

query_csv() {
	psql <<-EOF
	COPY ($1) to STDOUT with CSV DELIMITER ','
	EOF
}

query_influx() {
	arr2py=$(cat <<EOF
import sys
query_name = sys.argv[1]
fields = {x.split('=')[0]: int(x.split('=')[1]) for x in sys.argv[2].split(';')}
tags = []
if len(sys.argv) > 3 and len(sys.argv[3]) > 0:
	tags = {x.split('=')[0]: int(x.split('=')[1]) for x in sys.argv[3].split(';')}
for line in sys.stdin.read().split('\n'):
	if len(line) == 0:
		continue
	parsed = line.split(',')
	metric = query_name
	if len(tags):
		tag_data = ['%s=%s' % (k, parsed[v].replace(' ', '\\ '))  for (k, v) in tags.items()]
		metric += ',' + ','.join(tag_data)
	field_data = ['%s=%s' % (k, parsed[v])  for (k, v) in fields.items()]
	metric += ' ' + ','.join(field_data)
	print(metric)
EOF
)

	psql -c "COPY ($1) to STDOUT with CSV DELIMITER E','"| python -c "$arr2py" "$2" "$3" "$4"
}

gdpr_safe() {
	if [ -z "$GDPR_MODE"  ]; then
		echo "$1"
	else
		# Try and be privacy respecting while generating numbers that can allow
		# linking data across tables if need be?
		echo "substring(md5($1 || now()::date), 0, 12) as ${2:-$1}"
	fi
}
error() {
	(>&2 echo "$(tput setab 1)$@$(tput sgr0)")
}

warning() {
	(>&2 echo "$(tput setab 2)$@$(tput sgr0)")
}

success() {
	echo "$(tput setaf 40)$@$(tput sgr0)"
}
config_validate() { ## config validate: validate config files
	handle_help "$@" <<-EOF
		Validate the configuration files
		**Warning**:
		- This requires you to have \`\$GALAXY_DIST\` set and to have config under \`\$GALAXY_DIST/config\`.
		- This only validates that it is well formed XML, and does **not** validate against any schemas.

		    $ gxadmin validate
		      OK: galaxy-dist/data_manager_conf.xml
		      ...
		      OK: galaxy-dist/config/tool_data_table_conf.xml
		      OK: galaxy-dist/config/tool_sheds_conf.xml
		    All XML files validated
	EOF

	assert_set_env GALAXY_CONFIG_DIR
	assert_set_env GALAXY_MUTABLE_CONFIG_DIR

	fail_count=0
	for file in ${GALAXY_CONFIG_DIR}/*.xml; do
		xmllint $file > /dev/null 2>/dev/null;
		exit_code=$?
		if (( $exit_code > 0 )); then
			fail_count=$(echo "$fail_count + 1" | bc)
			error "  FAIL: $file ($exit_code)";
		else
			success "  OK: $file";
		fi
	done;

	for file in ${GALAXY_MUTABLE_CONFIG_DIR}/*.xml; do
		xmllint $file > /dev/null 2>/dev/null;
		exit_code=$?
		if (( $exit_code > 0 )); then
			fail_count=$(echo "$fail_count + 1" | bc)
			error "  FAIL: $file ($exit_code)";
		else
			success "  OK: $file";
		fi
	done;

	if (( fail_count == 0 )); then
		success "All XML files validated"
	else
		error "XML validation failed, cancelling any actions."
		exit 1
	fi
}

config_dump() { ## config dump: Dump Galaxy configuration as JSON
	handle_help "$@" <<-EOF
		This function was added with the intention to use it internally, but it may be useful in your workflows. It uses the python code from the Galaxy codebase in order to properly load the configuration which is then dumped as JSON.

		    (.venv)$ gxadmin dump-config | jq -S . | head
		    {
		      "activation_grace_period": 3,
		      "admin_users": "hxr@local.host",
		      "admin_users_list": [
		        "hxr@local.host"
		      ],
		      "allow_library_path_paste": false,
		      "allow_path_paste": false,
		      "allow_user_creation": true,
		      "allow_user_dataset_purge": true,
	EOF

	assert_set_env GALAXY_ROOT
	assert_set_env GALAXY_CONFIG_FILE

	dump_config_python=$(cat <<EOF
import argparse
import json
import os
import sys

sys.path.insert(1, '$GALAXY_ROOT/lib')

import galaxy.config
from galaxy.util.script import app_properties_from_args, populate_config_args

parser = argparse.ArgumentParser()
populate_config_args(parser)
args = parser.parse_args()
args.config_file = '$GALAXY_CONFIG_FILE'

app_properties = app_properties_from_args(args)
config = galaxy.config.Configuration(**app_properties)

sys.stdout.write(json.dumps(config.__dict__, default=lambda o: '<fail>'))
EOF
)
	echo "$dump_config_python" | python
}

#mutate_fail_job() { # mutate fail-job <job-id>: Cause a specific job and all of its outputs to be marked as failing
	#handle_help "$@" <<-EOF
	#EOF

	#commit="ROLLBACK;"
	##if [[ $1 == "--commit" ]]; then
		##commit="COMMIT;"
	##fi

	#read -r -d '' QUERY <<-EOF
		#BEGIN TRANSACTION;

		#UPDATE dataset
		#SET
			#state = 'error'
		#WHERE id in (select id from dataset where )

		#UPDATE history_dataset_association
		#SET
			#blurb = 'execution error',
			#info = 'This dataset''s job failed and has been manually addressed by a Galaxy administrator. Please use the bug icon to report this if you need assistance.'
		#WHERE id in (select hda_id from terminal_jobs_temp)

		#$COMMIT
	#EOF
#}

mutate_fail-terminal-datasets() { ## mutate fail-terminal-datasets [--commit]: Causes the output datasets of jobs which were manually failed, to be marked as failed
	handle_help "$@" <<-EOF
		Whenever an admin marks a job as failed manually (e.g. by updating the
		state in the database), the output datasets are not accordingly updated
		by default. And this causes users to mistakenly think their jobs are
		still running when they have long since failed.

		This command provides a way to select those jobs in error states
		(deleted, deleted_new, error, error_manually_dropped,
		new_manually_dropped), find their associated output datasets, and fail
		them with a blurb mentionining that they should contact the admin in
		case of any question

		Running without any arguments will execute the command within a
		transaction and then roll it back, allowing you to see counts of rows
		and giving you an idea if it is doing the right thing.

		**WARNINGS**

		This does NOT currently work on collections

		**EXAMPLES**

		The process is to first query how many datasets will be failed, if this looks correct you're ready to go.

		    $ gxadmin mutate fail-terminal-datasets
		    BEGIN
		    SELECT 1
		    jobs_per_month_to_be_failed | count
		    -----------------------------+-------
		    2019-02-01 00:00:00         |     1
		    (1 row)

		    UPDATE 1
		    UPDATE 1
		    ROLLBACK

		Then to run with the --commit flag to commit the changes

		    $ gxadmin mutate fail-terminal-datasets --commit
		    BEGIN
		    SELECT 1
		    jobs_per_month_to_be_failed | count
		    -----------------------------+-------
		    2019-02-01 00:00:00         |     1
		    (1 row)

		    UPDATE 1
		    UPDATE 1
		    COMMIT
	EOF
	# TODO(hxr): support collections

	commit="ROLLBACK;"
	if [[ $1 == "--commit" ]]; then
		commit="COMMIT;"
	fi

	read -r -d '' QUERY <<-EOF
		BEGIN TRANSACTION;

		CREATE TEMP TABLE terminal_jobs_temp AS
			SELECT
				dataset.id as ds_id,
				history_dataset_association.id as hda_id,
				dataset.create_time AT TIME ZONE 'UTC' as ds_create
			FROM
				dataset,
				history_dataset_association,
				job_to_output_dataset,
				job
			WHERE
				dataset.id = history_dataset_association.dataset_id
				AND history_dataset_association.id = job_to_output_dataset.dataset_id
				AND job.id = job_to_output_dataset.job_id
				AND dataset.state IN ('queued', 'running', 'new')
				AND job.state
					IN ('deleted', 'deleted_new', 'error', 'error_manually_dropped', 'new_manually_dropped');

		SELECT
			date_trunc('month', ds_create) as jobs_per_month_to_be_failed, count(*)
		FROM terminal_jobs_temp
		GROUP BY jobs_per_month_to_be_failed
		ORDER BY date_trunc('month', ds_create) desc;

		UPDATE dataset
		SET
			state = 'error'
		WHERE id in (select ds_id from terminal_jobs_temp);

		UPDATE history_dataset_association
		SET
			blurb = 'execution error',
			info = 'This dataset''s job failed and has been manually addressed by a Galaxy administrator. Please use the bug icon to report this if you need assistance.'
		WHERE id in (select hda_id from terminal_jobs_temp);

		$commit
	EOF
}

query_latest-users() { ## query latest-users: 40 recently registered users
	handle_help "$@" <<-EOF
		Returns 40 most recently registered users

		    $ gxadmin query latest-users
		     id |        create_time        | pg_size_pretty |   username    |             email
		    ----+---------------------------+----------------+---------------+--------------------------------
		      1 | 2018-10-05 11:40:42.90119 |                | helena-rasche | hxr@informatik.uni-freiburg.de
	EOF

	username=$(gdpr_safe username)
	email=$(gdpr_safe email)

	read -r -d '' QUERY <<-EOF
		SELECT
			id,
			create_time AT TIME ZONE 'UTC' as create_time,
			pg_size_pretty(disk_usage) as disk_usage,
			$username,
			$email,
			array_to_string(ARRAY(
				select galaxy_group.name from galaxy_group where id in (
					select group_id from user_group_association where user_group_association.user_id = galaxy_user.id
				)
			), ' ') as groups,
			active
		FROM galaxy_user
		ORDER BY create_time desc
		LIMIT 40
	EOF
}

query_tool-usage() { ## query tool-usage [weeks]: Counts of tool runs in the past weeks (default = all)
	handle_help "$@" <<-EOF
		    $ gxadmin tool-usage
		                                    tool_id                                 | count
		    ------------------------------------------------------------------------+--------
		     toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/1.1.0  | 958154
		     Grouping1                                                              | 638890
		     toolshed.g2.bx.psu.edu/repos/devteam/intersect/gops_intersect_1/1.0.0  | 326959
		     toolshed.g2.bx.psu.edu/repos/devteam/get_flanks/get_flanks1/1.0.0      | 320236
		     addValue                                                               | 313470
		     toolshed.g2.bx.psu.edu/repos/devteam/join/gops_join_1/1.0.0            | 312735
		     upload1                                                                | 103595
		     toolshed.g2.bx.psu.edu/repos/rnateam/graphclust_nspdk/nspdk_sparse/9.2 |  52861
		     Filter1                                                                |  43253
	EOF

	where=
	if (( $# > 0 )); then
		where="WHERE j.create_time > (now() - '$1 weeks'::interval)"
	fi

	fields="count=1"
	tags="tool_id=0"

	read -r -d '' QUERY <<-EOF
		SELECT
			j.tool_id, count(*) AS count
		FROM job j
		$where
		GROUP BY j.tool_id
		ORDER BY count DESC
	EOF
}

query_tool-popularity() { ## query tool-popularity [months|24]: Most run tools by month
	handle_help "$@" <<-EOF
		See most popular tools by month

		    $ ./gxadmin query tool-popularity 1
		              tool_id          |   month    | count
		    ---------------------------+------------+-------
		     circos                    | 2019-02-01 |    20
		     upload1                   | 2019-02-01 |    12
		     require_format            | 2019-02-01 |     9
		     circos_gc_skew            | 2019-02-01 |     7
		     circos_wiggle_to_scatter  | 2019-02-01 |     3
		     test_history_sanitization | 2019-02-01 |     2
		     circos_interval_to_tile   | 2019-02-01 |     1
		     __SET_METADATA__          | 2019-02-01 |     1
		    (8 rows)
	EOF

	fields="count=2"
	tags="tool_id=0;month=1"

	months=${1:-24}

	read -r -d '' QUERY <<-EOF
		SELECT
			tool_id,
			date_trunc('month', create_time AT TIME ZONE 'UTC')::date as month,
			count(*)
		FROM job
		WHERE create_time > (now() AT TIME ZONE 'UTC' - '$months months'::interval)
		GROUP BY tool_id, month
		ORDER BY month desc, count desc
	EOF
}

query_workflow-connections() { ## query workflow-connections [--all]: The connections of tools, from output to input, in the latest (or all) versions of user workflows
	handle_help "$@" <<-EOF
		This is used by the usegalaxy.eu tool prediction workflow, allowing for building models out of tool connections in workflows.

		    $ gxadmin query workflow-connections
		     wf_id |     wf_updated      | in_id |      in_tool      | in_tool_v | out_id |     out_tool      | out_tool_v
		    -------+---------------------+-------+-------------------+-----------+--------+-------------------+------------
		         3 | 2013-02-07 16:48:00 |     5 | Grep1             | 1.0.1     |     12 |                   |
		         3 | 2013-02-07 16:48:00 |     6 | Cut1              | 1.0.1     |      7 | Remove beginning1 | 1.0.0
		         3 | 2013-02-07 16:48:00 |     7 | Remove beginning1 | 1.0.0     |      5 | Grep1             | 1.0.1
		         3 | 2013-02-07 16:48:00 |     8 | addValue          | 1.0.0     |      6 | Cut1              | 1.0.1
		         3 | 2013-02-07 16:48:00 |     9 | Cut1              | 1.0.1     |      7 | Remove beginning1 | 1.0.0
		         3 | 2013-02-07 16:48:00 |    10 | addValue          | 1.0.0     |     11 | Paste1            | 1.0.0
		         3 | 2013-02-07 16:48:00 |    11 | Paste1            | 1.0.0     |      9 | Cut1              | 1.0.1
		         3 | 2013-02-07 16:48:00 |    11 | Paste1            | 1.0.0     |      8 | addValue          | 1.0.0
		         4 | 2013-02-07 16:48:00 |    13 | cat1              | 1.0.0     |     18 | addValue          | 1.0.0
		         4 | 2013-02-07 16:48:00 |    13 | cat1              | 1.0.0     |     20 | Count1            | 1.0.0
	EOF

	read -r -d '' wf_filter <<-EOF
	WHERE
		workflow.id in (
			SELECT
			 workflow.id
			FROM
			 stored_workflow
			LEFT JOIN
			 workflow on stored_workflow.latest_workflow_id = workflow.id
		)
	EOF
	if [[ $1 == "--all" ]]; then
		wf_filter=""
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			ws_in.workflow_id as wf_id,
			workflow.update_time AT TIME ZONE 'UTC' as wf_updated,
			wfc.input_step_id as in_id,
			ws_in.tool_id as in_tool,
			ws_in.tool_version as in_tool_v,
			wfc.output_step_id as out_id,
			ws_out.tool_id as out_tool,
			ws_out.tool_version as out_tool_v
		FROM
			workflow_step_connection wfc
		JOIN workflow_step ws_in  ON ws_in.id = wfc.input_step_id
		JOIN workflow_step ws_out ON ws_out.id = wfc.output_step_id
		JOIN workflow on ws_in.workflow_id = workflow.id
		$wf_filter
	EOF
}

query_datasets-created-daily() { ## query datasets-created-daily: The min/max/average/p95/p99 of total size of datasets created in a single day.
	handle_help "$@" <<-EOF
		    $ gxadmin query datasets-created-daily
		       min   |  avg   | perc_95 | perc_99 |  max
		    ---------+--------+---------+---------+-------
		     0 bytes | 338 GB | 1355 GB | 2384 GB | 42 TB
	EOF

	read -r -d '' QUERY <<-EOF
		WITH temp_queue_times AS
		(select
			date_trunc('day', create_time AT TIME ZONE 'UTC'),
			sum(total_size)
		from dataset
		group by date_trunc
		order by date_trunc desc)
		select
			pg_size_pretty(min(sum)) as min,
			pg_size_pretty(avg(sum)) as avg,
			pg_size_pretty(percentile_cont(0.95) WITHIN GROUP (ORDER BY sum) ::bigint) as perc_95,
			pg_size_pretty(percentile_cont(0.99) WITHIN GROUP (ORDER BY sum) ::bigint) as perc_99,
			pg_size_pretty(max(sum)) as max
		from temp_queue_times
	EOF
}

query_largest-collection() { ## query largest-collection: Returns the size of the single largest collection
	handle_help "$@" <<-EOF
	EOF

	fields="count=0"
	tags=""

	read -r -d '' QUERY <<-EOF
		WITH temp_table_collection_count AS (
			SELECT count(*)
			FROM dataset_collection_element
			GROUP BY dataset_collection_id
			ORDER BY count desc
		)
		select max(count) as count from temp_table_collection_count
	EOF
}

query_queue-time() { ## query queue-time <tool_id>: The average/95%/99% a specific tool spends in queue state.
	handle_help "$@" <<-EOF
		    $ gxadmin query queue-time toolshed.g2.bx.psu.edu/repos/nilesh/rseqc/rseqc_geneBody_coverage/2.6.4.3
		           min       |     perc_95     |     perc_99     |       max
		    -----------------+-----------------+-----------------+-----------------
		     00:00:15.421457 | 00:00:55.022874 | 00:00:59.974171 | 00:01:01.211995
	EOF

	assert_count $# 1 "Missing tool ID"

	read -r -d '' QUERY <<-EOF
		WITH temp_queue_times AS
		(select
			min(a.create_time - b.create_time) as queue_time
		from
			job_state_history as a
		inner join
			job_state_history as b
		on
			(a.job_id = b.job_id)
		where
			a.job_id in (select id from job where tool_id like '%$3%' and state = 'ok' and create_time > (now() AT TIME ZONE 'UTC' - '3 months'::interval))
			and a.state = 'running'
			and b.state = 'queued'
		group by
			a.job_id
		order by
			queue_time desc
		)
		select
			min(queue_time),
			percentile_cont(0.95) WITHIN GROUP (ORDER BY queue_time) as perc_95,
			percentile_cont(0.99) WITHIN GROUP (ORDER BY queue_time) as perc_99,
			max(queue_time)
		from temp_queue_times
	EOF
}

query_queue() { ## query queue: Brief overview of currently running jobs
	handle_help "$@" <<-EOF
		    $ gxadmin query queue
		                                tool_id                                |  state  | count
		    -------------------------------------------------------------------+---------+-------
		     toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0      | queued  |     9
		     toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0     | running |     7
		     toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                | queued  |     6
		     ebi_sra_main                                                      | running |     6
		     toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.8.3            | queued  |     5
		     toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.3.4.2      | running |     5
		     toolshed.g2.bx.psu.edu/repos/nml/spades/spades/3.11.1+galaxy1     | queued  |     4
		     toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0 | running |     2
		     toolshed.g2.bx.psu.edu/repos/nml/metaspades/metaspades/3.9.0      | running |     2
		     upload1                                                           | running |     2
	EOF

	fields="count=2"
	tags="tool_id=0;state=1"

	read -r -d '' QUERY <<-EOF
			SELECT tool_id, state, count(*)
			FROM job
			WHERE state in ('queued', 'running')
			GROUP BY tool_id, state
			ORDER BY count desc
	EOF
}

query_queue-overview() { ## query queue-overview [--short-tool-id]: View used mostly for monitoring
	handle_help "$@" <<-EOF
		Primarily for monitoring of queue. Optimally used with 'iquery' and passed to Telegraf.

		    $ gxadmin iquery queue-overview
		    queue-overview,tool_id=upload1,tool_version=0.0.1,state=running,handler=main.web.1,destination_id=condor,job_runner_name=condor,user=1 count=1

	EOF

	# Use full tool id by default
	tool_id="tool_id"
	if [[ $1 = --short-tool-id ]]; then
		tool_id="regexp_replace(tool_id, '.*toolshed.*/repos/', '')"
	fi

	# Include by default
	if [ -z "$GDPR_MODE"  ]; then
		user_id='user_id'
	else
		user_id="'0'"
	fi

	fields="count=6"
	tags="tool_id=0;tool_version=1;destination_id=2;handler=3;state=4;job_runner_name=5;user_id=7"

	read -r -d '' QUERY <<-EOF
		SELECT
			regexp_replace($tool_id, '/[0-9.a-z+-]+$', '') as tool_id,
			tool_version,
			COALESCE(destination_id, 'unknown'),
			COALESCE(handler, 'unknown'),
			state,
			COALESCE(job_runner_name, 'unknown'),
			count(*) as count,
			$user_id as user_id
		FROM job
		WHERE
			state = 'running' or state = 'queued' or state = 'new'
		GROUP BY
			tool_id, tool_version, destination_id, handler, state, job_runner_name, user_id
	EOF
}

query_queue-detail() { ## query queue-detail [--all]: Detailed overview of running and queued jobs
	handle_help "$@" <<-EOF
		    $ gxadmin query queue-detail
		      state  |   id    |  extid  |                                 tool_id                                   | username | time_since_creation
		    ---------+---------+---------+---------------------------------------------------------------------------+----------+---------------------
		     running | 4360629 | 229333  | toolshed.g2.bx.psu.edu/repos/bgruening/infernal/infernal_cmsearch/1.1.2.0 | xxxx     | 5 days 11:00:00
		     running | 4362676 | 230237  | toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0         | xxxx     | 4 days 18:00:00
		     running | 4364499 | 231055  | toolshed.g2.bx.psu.edu/repos/iuc/mothur_venn/mothur_venn/1.36.1.0         | xxxx     | 4 days 05:00:00
		     running | 4366604 | 5183013 | toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0             | xxxx     | 3 days 20:00:00
		     running | 4366605 | 5183016 | toolshed.g2.bx.psu.edu/repos/iuc/dexseq/dexseq_count/1.24.0.0             | xxxx     | 3 days 20:00:00
		     queued  | 4350274 | 225743  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              | xxxx     | 9 days 05:00:00
		     queued  | 4353435 | 227038  | toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/2.8.3                    | xxxx     | 8 days 08:00:00
		     queued  | 4361914 | 229712  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              | xxxx     | 5 days -01:00:00
		     queued  | 4361812 | 229696  | toolshed.g2.bx.psu.edu/repos/iuc/unicycler/unicycler/0.4.6.0              | xxxx     | 5 days -01:00:00
		     queued  | 4361939 | 229728  | toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                        | xxxx     | 4 days 21:00:00
		     queued  | 4361941 | 229731  | toolshed.g2.bx.psu.edu/repos/nml/spades/spades/1.2                        | xxxx     | 4 days 21:00:00
	EOF

	d=""
	if [[ $1 == "--all" ]]; then
		d=", 'new'"
	fi

	username=$(gdpr_safe galaxy_user.username username)

	read -r -d '' QUERY <<-EOF
		SELECT
			job.state,
			job.id,
			job.job_runner_external_id as extid,
			job.tool_id,
			$username,
			(now() AT TIME ZONE 'UTC' - job.create_time) as time_since_creation,
			job.handler,
			job.job_runner_name,
			job.destination_id
		FROM job, galaxy_user
		WHERE
			state in ('running', 'queued'$d)
			AND job.user_id = galaxy_user.id
		ORDER BY
			state desc,
			time_since_creation desc
	EOF
}

query_runtime-per-user() { ## query runtime-per-user <email>: computation time of user (by email)
	handle_help "$@" <<-EOF
		    $ gxadmin query runtime-per-user hxr@informatik.uni-freiburg.de
		       sum
		    ----------
		     14:07:39
	EOF

	assert_count $# 1 "Missing user"

	read -r -d '' QUERY <<-EOF
			SELECT sum((metric_value || ' second')::interval)
			FROM job_metric_numeric
			WHERE job_id in (
				SELECT id
				FROM job
				WHERE user_id in (
					SELECT id
					FROM galaxy_user
					where email = '$1'
				)
			) AND metric_name = 'runtime_seconds'
	EOF
}

query_jobs-nonterminal() { ## query jobs-nonterminal [username|id|email]: Job info of nonterminal jobs separated by user
	handle_help "$@" <<-EOF
		You can request the user information by username, id, and user email

		    $ gxadmin query jobs-nonterminal helena-rasche
		       id    | tool_id             |  state  |        create_time         | runner | id     |     handler     | user_id
		    ---------+---------------------+---------+----------------------------+--------+--------+-----------------+---------
		     4760549 | featurecounts/1.6.3 | running | 2019-01-18 14:05:14.871711 | condor | 197549 | handler_main_7  | 599
		     4760552 | featurecounts/1.6.3 | running | 2019-01-18 14:05:16.205867 | condor | 197552 | handler_main_7  | 599
		     4760554 | featurecounts/1.6.3 | running | 2019-01-18 14:05:17.170157 | condor | 197580 | handler_main_8  | 599
		     4760557 | featurecounts/1.6.3 | running | 2019-01-18 14:05:18.25044  | condor | 197545 | handler_main_10 | 599
		     4760573 | featurecounts/1.6.3 | running | 2019-01-18 14:05:47.20392  | condor | 197553 | handler_main_2  | 599
		     4760984 | deseq2/2.11.40.4    | new     | 2019-01-18 14:56:37.700714 |        |        | handler_main_1  | 599
		     4766092 | deseq2/2.11.40.4    | new     | 2019-01-21 07:24:16.232376 |        |        | handler_main_5  | 599
		     4811598 | cuffnorm/2.2.1.2    | running | 2019-02-01 13:08:30.400016 | condor | 248432 | handler_main_0  | 599
		    (8 rows)

		You can also query all non-terminal jobs by all users

		    $ gxadmin query jobs-nonterminal | head
		       id    |  tool_id            |  state  |        create_time         | runner | id     |     handler     | user_id
		    ---------+---------------------+---------+----------------------------+--------+--------+-----------------+---------
		     4760549 | featurecounts/1.6.3 | running | 2019-01-18 14:05:14.871711 | condor | 197549 | handler_main_7  |     599
		     4760552 | featurecounts/1.6.3 | running | 2019-01-18 14:05:16.205867 | condor | 197552 | handler_main_7  |     599
		     4760554 | featurecounts/1.6.3 | running | 2019-01-18 14:05:17.170157 | condor | 197580 | handler_main_8  |     599
		     4760557 | featurecounts/1.6.3 | running | 2019-01-18 14:05:18.25044  | condor | 197545 | handler_main_10 |     599
		     4760573 | featurecounts/1.6.3 | running | 2019-01-18 14:05:47.20392  | condor | 197553 | handler_main_2  |     599
		     4760588 | featurecounts/1.6.3 | new     | 2019-01-18 14:11:03.766558 |        |        | handler_main_9  |      11
		     4760589 | featurecounts/1.6.3 | new     | 2019-01-18 14:11:05.895232 |        |        | handler_main_1  |      11
		     4760590 | featurecounts/1.6.3 | new     | 2019-01-18 14:11:07.328533 |        |        | handler_main_2  |      11
	EOF

	if (( $# > 0 )); then
		user_filter="(galaxy_user.email = '$1' or galaxy_user.username = '$1' or galaxy_user.id = CAST(REGEXP_REPLACE(COALESCE('$1','0'), '[^0-9]+', '0', 'g') AS INTEGER))"
	else
		user_filter="true"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			job.id, job.tool_id, job.state, job.create_time AT TIME ZONE 'UTC', job.job_runner_name, job.job_runner_external_id, job.handler, job.user_id
		FROM
			job
		JOIN
			galaxy_user ON galaxy_user.id = job.user_id
		WHERE
			$user_filter AND job.state IN ('new', 'queued', 'running')
		ORDER BY job.id ASC
	EOF
}

query_jobs-per-user() { ## query jobs-per-user <email>: Number of jobs run by a specific user
	handle_help "$@" <<-EOF
		    $ gxadmin query jobs-per-user hxr@informatik.uni-freiburg.de
		     count
		    -------
		      1460
	EOF

	assert_count $# 1 "Missing user"
	read -r -d '' QUERY <<-EOF
			SELECT count(id)
			FROM job
			WHERE user_id in (
				SELECT id
				FROM galaxy_user
				WHERE email = '$1')
	EOF
}

query_recent-jobs() { ## query recent-jobs <hours>: Jobs run in the past <hours> (in any state)
	handle_help "$@" <<-EOF
		    $ gxadmin query recent-jobs 2.1
		       id    |     create_time     |      tool_id          | state |    username
		    ---------+---------------------+-----------------------+-------+-----------------
		     4383997 | 2018-10-05 16:07:00 | Filter1               | ok    |
		     4383994 | 2018-10-05 16:04:00 | echo_main_condor      | ok    |
		     4383993 | 2018-10-05 16:04:00 | echo_main_drmaa       | error |
		     4383992 | 2018-10-05 16:04:00 | echo_main_handler11   | ok    |
		     4383983 | 2018-10-05 16:04:00 | echo_main_handler2    | ok    |
		     4383982 | 2018-10-05 16:04:00 | echo_main_handler1    | ok    |
		     4383981 | 2018-10-05 16:04:00 | echo_main_handler0    | ok    |
	EOF

	assert_count $# 1 "Missing hours"

	username=$(gdpr_safe galaxy_user.username username)

	read -r -d '' QUERY <<-EOF
		SELECT
			job.id,
			job.create_time AT TIME ZONE 'UTC' as create_time,
			job.tool_id,
			job.state, $username
		FROM job, galaxy_user
		WHERE job.create_time > (now() AT TIME ZONE 'UTC' - '$1 hours'::interval) AND job.user_id = galaxy_user.id
		ORDER BY id desc
	EOF
}

query_training-list() { ## query training-list [--all]: List known trainings
	handle_help "$@" <<-EOF
		This module is specific to EU's implementation of Training Infrastructure as a Service. But this specifically just checks for all groups with the name prefix 'training-'

		    $ gxadmin query training
		           name       |  created
		    ------------------+------------
		     hts2018          | 2018-09-19
	EOF

	d1=""
	d2="AND deleted = false"
	if [[ $1 == "--all" ]]; then
		d1=", deleted"
		d2=""
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			substring(name from 10) as name,
			date_trunc('day', create_time AT TIME ZONE 'UTC')::date as created
			$d1
		FROM galaxy_group
		WHERE name like 'training-%' $d2
		ORDER BY create_time DESC
	EOF
}

query_training-members() { ## query training-members <tr_id>: List users in a specific training
	handle_help "$@" <<-EOF
		    $ gxadmin query training-members hts2018
		          username      |       joined
		    --------------------+---------------------
		     helena-rasche      | 2018-09-21 21:42:01
	EOF

	assert_count $# 1 "Missing Training ID"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')
	username=$(gdpr_safe galaxy_user.username username)

	read -r -d '' QUERY <<-EOF
			SELECT
				$username,
				date_trunc('second', user_group_association.create_time AT TIME ZONE 'UTC') as joined
			FROM galaxy_user, user_group_association, galaxy_group
			WHERE galaxy_group.name = 'training-$ww'
				AND galaxy_group.id = user_group_association.group_id
				AND user_group_association.user_id = galaxy_user.id
	EOF
}

query_training-members-remove() { ## query training-members-remove <training> <username> [YESDOIT]: Remove a user from a training
	handle_help "$@" <<-EOF
	EOF

	assert_count_ge $# 2 "Missing parameters"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	if (( $# == 3 )) && [[ "$3" == "YESDOIT" ]]; then
		results="$(query_tsv "$qstr")"
		uga_id=$(echo "$results" | awk -F'\t' '{print $1}')
		if (( uga_id > -1 )); then
			qstr="delete from user_group_association where id = $uga_id"
		fi
		echo $qstr
	else
		read -r -d '' QUERY <<-EOF
			SELECT
				user_group_association.id,
				galaxy_user.username,
				galaxy_group.name
			FROM
				user_group_association
			LEFT JOIN galaxy_user ON user_group_association.user_id = galaxy_user.id
			LEFT JOIN galaxy_group ON galaxy_group.id = user_group_association.group_id
			WHERE
				galaxy_group.name = 'training-$ww'
				AND galaxy_user.username = '$2'
		EOF
	fi
}

query_largest-histories() { ## query largest-histories: Largest histories in Galaxy
	handle_help "$@" <<-EOF
		Finds all jobs by people in that queue (including things they are executing that are not part of a training)

		    $ gxadmin query largest-histories
		     total_size | id | substring  | username
		    ------------+----+------------+----------
		     50 MB      |  6 | Unnamed hi | helena
		     41 MB      |  8 | Unnamed hi | helena
		     35 MB      |  9 | Unnamed hi | helena
		     27 MB      | 10 | Circos     | helena
		     3298 kB    |  2 | Tag Testin | helena
		     9936 bytes | 44 | test       | helena
		     413 bytes  | 45 | Unnamed hi | alice
	EOF

	username=$(gdpr_safe galaxy_user.username username)

	read -r -d '' QUERY <<-EOF
		SELECT
			pg_size_pretty(sum(coalesce(dataset.total_size, 0))) as total_size,
			history.id,
			substring(history.name, 1, 10),
			$username
		FROM
			dataset
			JOIN history_dataset_association on dataset.id = history_dataset_association.dataset_id
			JOIN history on history_dataset_association.history_id = history.id
			JOIN galaxy_user on history.user_id = galaxy_user.id
		GROUP BY history.id, history.name, history.user_id, galaxy_user.username
		ORDER BY sum(coalesce(dataset.total_size, 0)) DESC
	EOF
}

query_training-queue() { ## query training-queue <training_id>: Jobs currently being run by people in a given training
	handle_help "$@" <<-EOF
		Finds all jobs by people in that queue (including things they are executing that are not part of a training)

		    $ gxadmin query training-queue hts2018
		     state  |   id    | extid  | tool_id |   username    |       created
		    --------+---------+--------+---------+---------------+---------------------
		     queued | 4350274 | 225743 | upload1 |               | 2018-09-26 10:00:00
	EOF

	assert_count $# 1 "Missing Training ID"
	# Remove training- if they used it.
	ww=$(echo "$1" | sed 's/^training-//g')

	read -r -d '' QUERY <<-EOF
			SELECT
				job.state,
				job.id,
				job.job_runner_external_id AS extid,
				job.tool_id,
				galaxy_user.username,
				job.create_time AT TIME ZONE 'UTC' AS created
			FROM
				job, galaxy_user
			WHERE
				job.user_id = galaxy_user.id
				AND job.create_time > (now() AT TIME ZONE 'UTC' - '3 hours'::interval)
				AND galaxy_user.id
					IN (
							SELECT
								galaxy_user.id
							FROM
								galaxy_user, user_group_association, galaxy_group
							WHERE
								galaxy_group.name = 'training-$ww'
								AND galaxy_group.id = user_group_association.group_id
								AND user_group_association.user_id = galaxy_user.id
						)
			ORDER BY
				job.create_time ASC
	EOF
}

query_disk-usage() { ## query disk-usage [--nice]: Disk usage per object store.
	handle_help "$@" <<-EOF
		Query the different object stores referenced in your Galaxy database

		    $ gxadmin query disk-usage
		     object_store_id |      sum
		    -----------------+----------------
		     files8          | 88109503720067
		     files6          | 64083627169725
		     files9          | 53690953947700
		     files7          | 30657241908566

		Or you can supply the --nice flag, but this should not be used with iquery/InfluxDB

		    $ gxadmin query disk-usage --nice
		     object_store_id |   sum
		    -----------------+---------
		     files9          | 114 TB
		     files8          | 77 TB
		     files7          | 56 TB
		     files6          | 17 TB
	EOF

	fields="count=1"
	tags="object_store_id=0"

	size="sum(total_size)"
	if [[ $1 == "--nice" ]]; then
		size="pg_size_pretty(sum(total_size)) as sum"
	fi

	read -r -d '' QUERY <<-EOF
			SELECT
				object_store_id, $size
			FROM dataset
			WHERE NOT purged
			GROUP BY object_store_id
			ORDER BY sum(total_size) DESC
	EOF
}

query_users-count() { ## query users-count: Shows sums of active/external/deleted/purged accounts
	handle_help "$@" <<-EOF
		     active | external | deleted | purged | count
		    --------+----------+---------+--------+-------
		     f      | f        | f       | f      |   182
		     t      | f        | t       | t      |     2
		     t      | f        | t       | f      |     6
		     t      | f        | f       | f      |  2350
		     f      | f        | t       | t      |    36
	EOF

	fields="count=4"
	tags="active=0;external=1;deleted=2;purged=3"

	read -r -d '' QUERY <<-EOF
			SELECT
				active, external, deleted, purged, count(*) as count
			FROM
				galaxy_user
			GROUP BY
				active, external, deleted, purged
	EOF
}

query_tool-last-used-date() { ## query tool-last-used-date: When was the most recent invocation of every tool
	handle_help "$@" <<-EOF
		Example invocation:

		    $ gxadmin query tool-last-used-date
		             max         |          tool_id
		    ---------------------+---------------------------
		     2019-02-01 00:00:00 | test_history_sanitization
		     2018-12-01 00:00:00 | require_format
		     2018-11-01 00:00:00 | upload1
		    (3 rows)

		**WARNING**

		It is not truly every tool, there is no easy way to find the tools which have never been run.
	EOF

	read -r -d '' QUERY <<-EOF
		select max(date_trunc('month', create_time AT TIME ZONE 'UTC')), tool_id from job group by tool_id order by max desc
	EOF
}

query_users-total() { ## query users-total: Total number of Galaxy users (incl deleted, purged, inactive)
	handle_help "$@" <<-EOF
	EOF

	fields="count=0"
	tags=""

	read -r -d '' QUERY <<-EOF
			SELECT count(*) FROM galaxy_user
	EOF
}

query_groups-list() { ## query groups-list: List all groups known to Galaxy
	handle_help "$@" <<-EOF
	EOF

	fields="count=1"
	tags="group_name=0"

	read -r -d '' QUERY <<-EOF
			SELECT
				galaxy_group.name, count(*)
			FROM
				galaxy_group, user_group_association
			WHERE
				user_group_association.group_id = galaxy_group.id
			GROUP BY name
	EOF
}

query_collection-usage() { ## query collection-usage: Information about how many collections of various types are used
	handle_help "$@" <<-EOF
	EOF

	fields="count=1"
	tags="group_name=0"

	read -r -d '' QUERY <<-EOF
		SELECT
			dc.collection_type, count(*)
		FROM
			history_dataset_collection_association as hdca
		INNER JOIN
			dataset_collection as dc
			ON hdca.collection_id = dc.id
		GROUP BY
			dc.collection_type
	EOF
}

query_ts-repos() { ## query ts-repos: Counts of toolshed repositories by toolshed and owner.
	handle_help "$@" <<-EOF
	EOF

	fields="count=2"
	tags="tool_shed=0;owner=1"

	read -r -d '' QUERY <<-EOF
			SELECT
				tool_shed, owner, count(*)
			FROM
				tool_shed_repository
			GROUP BY
				tool_shed, owner
	EOF
}

query_active-users() { ## query active-users [weeks]: Deprecated, use monthly-users-active
	error "Deprecated, use monthly-users-active"
}

query_tool-metrics() { ## query tool-metrics <tool_id> <metric_id> [--like]: See values of a specific metric
	handle_help "$@" <<-EOF
		A good way to use this is to fetch the memory usage of a tool and then
		do some aggregations. The following requires [data_hacks](https://github.com/bitly/data_hacks)

		    $ gxadmin tsvquery tool-metrics %rgrnastar/rna_star% memory.max_usage_in_bytes --like | \\
		        awk '{print \$1 / 1024 / 1024 / 1024}' | \\
		        histogram.py --percentage
		    # NumSamples = 441; Min = 2.83; Max = 105.88
		    # Mean = 45.735302; Variance = 422.952289; SD = 20.565804; Median 51.090900
		    # each ∎ represents a count of 1
		        2.8277 -    13.1324 [    15]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (3.40%)
		       13.1324 -    23.4372 [    78]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (17.69%)
		       23.4372 -    33.7419 [    47]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (10.66%)
		       33.7419 -    44.0466 [    31]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (7.03%)
		       44.0466 -    54.3514 [    98]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (22.22%)
		       54.3514 -    64.6561 [   102]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (23.13%)
		       64.6561 -    74.9608 [    55]: ∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎ (12.47%)
		       74.9608 -    85.2655 [    11]: ∎∎∎∎∎∎∎∎∎∎∎ (2.49%)
		       85.2655 -    95.5703 [     3]: ∎∎∎ (0.68%)
		       95.5703 -   105.8750 [     1]: ∎ (0.23%)
	EOF

	assert_count_ge $# 1 "Missing Tool ID"
	assert_count_ge $# 2 "Missing Metric ID (hint: tool-available-metrics)"

	tool_subquery="SELECT id FROM job WHERE tool_id = '$1'"
	if [[ "$3" == "--like" ]]; then
		tool_subquery="SELECT id FROM job WHERE tool_id like '$1'"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			metric_value
		FROM job_metric_numeric
		WHERE
			metric_name = '$2'
			and
			job_id in (
				$tool_subquery
			)
	EOF
}

query_tool-available-metrics() { ## query tool-available-metrics <tool_id>: list all available metrics for a given tool
	handle_help "$@" <<-EOF
		Gives a list of available metrics, which can then be used to query.

		    [galaxy@sn04 galaxy]$ gxadmin query tool-available-metrics upload1
		                 metric_name
		    -------------------------------------
		     memory.stat.total_rss
		     memory.stat.total_swap
		     memory.stat.total_unevictable
		     memory.use_hierarchy
		     ...
	EOF

	assert_count $# 1 "Missing Tool ID"

	read -r -d '' QUERY <<-EOF
		SELECT
			distinct metric_name
		FROM job_metric_numeric
		WHERE job_id in (
			SELECT id FROM job WHERE tool_id = '$1'
		)
		ORDER BY metric_name asc
	EOF
}

query_monthly-cpu-years() { ## query monthly-cpu-years: CPU years allocated to tools by month
	handle_help "$@" <<-EOF
		This uses the galaxy_slots and runtime_seconds metrics in order to
		calculate allocated CPU years. This will not be the value of what is
		actually consumed by your jobs, you should use cgroups.

		    $ gxadmin query monthly-cpu-years
		         date_trunc      | cpu_years
		    ---------------------+-----------
		     2019-04-01 00:00:00 |      2.95
		     2019-03-01 00:00:00 |     12.38
		     2019-02-01 00:00:00 |     11.47
		     2019-01-01 00:00:00 |      8.27
		     2018-12-01 00:00:00 |     11.42
		     2018-11-01 00:00:00 |     16.99
		     2018-10-01 00:00:00 |     12.09
		     2018-09-01 00:00:00 |      6.27
		     2018-08-01 00:00:00 |      9.06
		     2018-07-01 00:00:00 |      6.17
		     2018-06-01 00:00:00 |      5.73
		     2018-05-01 00:00:00 |      7.36
		     2018-04-01 00:00:00 |     10.21
		     2018-03-01 00:00:00 |      5.20
		     2018-02-01 00:00:00 |      4.53
		     2018-01-01 00:00:00 |      4.05
		     2017-12-01 00:00:00 |      2.44
	EOF

	read -r -d '' QUERY <<-EOF
		SELECT
			date_trunc('month', job.create_time),
			round(sum((a.metric_value * b.metric_value) / 3600 / 24 / 365), 2) as cpu_years
		FROM
			job_metric_numeric a,
			job_metric_numeric b,
			job
		WHERE
			b.job_id = a.job_id
			AND a.job_id = job.id
			AND a.metric_name = 'runtime_seconds'
			AND b.metric_name = 'galaxy_slots'
		GROUP BY date_trunc('month', job.create_time)
		ORDER BY date_trunc('month', job.create_time) DESC
	EOF
}


query_monthly-data(){ ## query monthly-data [year]: Number of active users per month, running jobs
	handle_help "$@" <<-EOF
		Find out how much data was ingested or created by Galaxy during the past months.

		    $ gxadmin query monthly-data 2018
		     pg_size_pretty |        month
		    ----------------+---------------------
		     62 TB          | 2018-12-01 00:00:00
		     50 TB          | 2018-11-01 00:00:00
		     59 TB          | 2018-10-01 00:00:00
		     32 TB          | 2018-09-01 00:00:00
		     26 TB          | 2018-08-01 00:00:00
		     42 TB          | 2018-07-01 00:00:00
		     34 TB          | 2018-06-01 00:00:00
		     33 TB          | 2018-05-01 00:00:00
		     27 TB          | 2018-04-01 00:00:00
		     32 TB          | 2018-03-01 00:00:00
		     18 TB          | 2018-02-01 00:00:00
		     16 TB          | 2018-01-01 00:00:00
	EOF

	if (( $# > 0 )); then
		where="WHERE date_trunc('year', dataset.create_time AT TIME ZONE 'UTC') = '$1-01-01'::date"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			pg_size_pretty(sum(total_size)),
			date_trunc('month', dataset.create_time AT TIME ZONE 'UTC') AS month
		FROM
			dataset
		$where
		GROUP BY
			month
		ORDER BY
			month DESC
	EOF
}

query_monthly-users-registered(){ ## query monthly-users-registered [year]: Number of users registered each month
	handle_help "$@" <<-EOF
	EOF

	if (( $# > 0 )); then
		where="WHERE date_trunc('year', galaxy_user.create_time AT TIME ZONE 'UTC') = '$1-01-01'::date"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			date_trunc('month', galaxy_user.create_time)::DATE AS month,
			count(*)
		FROM
			galaxy_user
		$where
		GROUP BY
			month
		ORDER BY
			month DESC
	EOF
}

query_monthly-users() { ## query monthly-users: Deprecated, use monthly-users-active
	error "Deprecated, use monthly-users-active"
}

query_monthly-users-active(){ ## query monthly-users-active [year]: Number of active users per month, running jobs
	handle_help "$@" <<-EOF
		Number of unique users each month who ran jobs. **NOTE**: does not include anonymous users.

		    [galaxy@sn04 galaxy]$ gxadmin query monthly-users-active 2018
		       month    | active_users
		    ------------+--------------
		     2018-12-01 |          811
		     2018-11-01 |          658
		     2018-10-01 |          583
		     2018-09-01 |          444
		     2018-08-01 |          342
		     2018-07-01 |          379
		     2018-06-01 |          370
		     2018-05-01 |          330
		     2018-04-01 |          274
		     2018-03-01 |          186
		     2018-02-01 |          168
		     2018-01-01 |          122
	EOF

	if (( $# > 0 )); then
		where="WHERE date_trunc('year', job.create_time AT TIME ZONE 'UTC') = '$1-01-01'::date"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			date_trunc('month', job.create_time AT TIME ZONE 'UTC')::date as month,
			count(distinct user_id) as active_users
		FROM job
		$where
		GROUP BY month
		ORDER BY month DESC
	EOF
}

query_monthly-jobs(){ ## query monthly-jobs [year]: Number of jobs run each month
	handle_help "$@" <<-EOF
		Count jobs run each month

		    [galaxy@sn04 galaxy]$ gxadmin query monthly-jobs 2018
		        month   | count
		    ------------+--------
		     2018-12-01 |  96941
		     2018-11-01 |  94625
		     2018-10-01 | 156940
		     2018-09-01 | 103331
		     2018-08-01 | 128658
		     2018-07-01 |  90852
		     2018-06-01 | 230470
		     2018-05-01 | 182331
		     2018-04-01 | 109032
		     2018-03-01 | 197125
		     2018-02-01 | 260931
		     2018-01-01 |  25378
	EOF

	if (( $# > 0 )); then
		where="WHERE date_trunc('year', job.create_time AT TIME ZONE 'UTC') = '$1-01-01'::date"
	fi

	read -r -d '' QUERY <<-EOF
		SELECT
			date_trunc('month', job.create_time AT TIME ZONE 'UTC')::DATE AS month,
			count(*)
		FROM
			job
		$where
		GROUP BY
			month
		ORDER BY
			month DESC
	EOF

}

query_job-history() { ## query job-history <id>: Job state history for a specific job
	handle_help "$@" <<-EOF
		    $ gxadmin query job-history 4384025
		            time         |  state
		    ---------------------+---------
		     2018-10-05 16:20:13 | ok
		     2018-10-05 16:19:57 | running
		     2018-10-05 16:19:55 | queued
		     2018-10-05 16:19:54 | new
		    (4 rows)
	EOF

	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				create_time AT TIME ZONE 'UTC' as time,
				state
			FROM job_state_history
			WHERE job_id = $1
	EOF
}

query_job-inputs() { ## query job-inputs <id>: Input datasets to a specific job
	handle_help "$@" <<-EOF
	EOF
	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				hda.id AS hda_id,
				hda.state AS hda_state,
				hda.deleted AS hda_deleted,
				hda.purged AS hda_purged,
				d.id AS d_id,
				d.state AS d_state,
				d.deleted AS d_deleted,
				d.purged AS d_purged,
				d.object_store_id AS object_store_id
			FROM job j
				JOIN job_to_input_dataset jtid
					ON j.id = jtid.job_id
				JOIN history_dataset_association hda
					ON hda.id = jtid.dataset_id
				JOIN dataset d
					ON hda.dataset_id = d.id
			WHERE j.id = $1
	EOF
}

query_job-outputs() { ## query job-outputs <id>: Output datasets from a specific job
	handle_help "$@" <<-EOF
	EOF

	assert_count $# 1 "Missing Job ID"

	read -r -d '' QUERY <<-EOF
			SELECT
				hda.id AS hda_id,
				hda.state AS hda_state,
				hda.deleted AS hda_deleted,
				hda.purged AS hda_purged,
				d.id AS d_id,
				d.state AS d_state,
				d.deleted AS d_deleted,
				d.purged AS d_purged,
				d.object_store_id AS object_store_id
			FROM job j
				JOIN job_to_output_dataset jtod
					ON j.id = jtod.job_id
				JOIN history_dataset_association hda
					ON hda.id = jtod.dataset_id
				JOIN dataset d
					ON hda.dataset_id = d.id
			WHERE j.id = $1
		"
	EOF
}

query_old-histories(){ ## query old-histories <weeks>: Lists histories that haven't been updated (used) for <weeks>
	handle_help "$@" <<-EOF
		Histories and their users who haven't been updated for a specified number of weeks. Default number of weeks is 15.

		    $gxadmin query old-histories 52
		      id   |        update_time         | user_id |  email  |       name         | published | deleted | purged | hid_counter
		    -------+----------------------------+---------+---------+--------------------+-----------+---------+--------+-------------
		     39903 | 2017-06-13 12:35:07.174749 |     834 | xxx@xxx | Unnamed history    | f         | f       | f      |          23
		      1674 | 2017-06-13 14:08:30.017574 |       9 | xxx@xxx | SAHA project       | f         | f       | f      |          47
		     40088 | 2017-06-15 04:10:48.879122 |     986 | xxx@xxx | Unnamed history    | f         | f       | f      |           3
		     39023 | 2017-06-15 09:33:12.007002 |     849 | xxx@xxx | prac 4 new final   | f         | f       | f      |         297
		     35437 | 2017-06-16 04:41:13.15785  |     731 | xxx@xxx | Unnamed history    | f         | f       | f      |          98
		     40123 | 2017-06-16 13:43:24.948344 |     987 | xxx@xxx | Unnamed history    | f         | f       | f      |          22
		     40050 | 2017-06-19 00:46:29.032462 |     193 | xxx@xxx | Telmatactis        | f         | f       | f      |          74
		     12212 | 2017-06-20 14:41:03.762881 |     169 | xxx@xxx | Unnamed history    | f         | f       | f      |          24
		     39523 | 2017-06-21 01:34:52.226653 |       9 | xxx@xxx | OSCC Cell Lines    | f         | f       | f      |         139
	EOF

	assert_count_ge $# 1 "Missing <weeks>"

	weeks=$1
	email=$(gdpr_safe galaxy_user.email 'email')

	read -r -d '' QUERY <<-EOF
		SELECT
			history.id,
			history.update_time AT TIME ZONE 'UTC' as update_time,
			history.user_id,
			$email,
			history.name,
			history.published,
			history.deleted,
			history.purged,
			history.hid_counter
		FROM
			history,
			galaxy_user
		WHERE
			history.update_time < (now() AT TIME ZONE 'UTC' - '$weeks weeks'::interval) AND
			history.user_id = galaxy_user.id AND
			history.deleted = FALSE AND
			history.published = FALSE
		ORDER BY
			history.update_time desc
	EOF
}

# TODO(hxr): generic summation by metric? Leave math to consumer?
query_jobs-max-by-cpu-hours() { ## query jobs-max-by-cpu-hours: Top 10 jobs by CPU hours consumed (requires CGroups metrics)
	handle_help "$@" <<-EOF
	EOF

	read -r -d '' QUERY <<-EOF
		SELECT
			job.id,
			job.tool_id,
			job.create_time,
			metric_value/1000000000/3600/24 as cpu_days
		FROM job, job_metric_numeric
		WHERE
			job.id = job_metric_numeric.job_id
			AND metric_name = 'cpuacct.usage'
		ORDER BY cpu_hours desc
		LIMIT 30
	EOF
}

query_errored-jobs(){ ## query errored-jobs <hours>: Lists jobs that errored in the last N hours.
	handle_help "$@" <<-EOF
		Lists details of jobs that have status = 'error' for the specified number of hours. Default = 24 hours

		    $gxadmin query errored-jobs 24
		    TO_DO: Add output of query here!

	EOF

	hours=$1
	email=$(gdpr_safe galaxy_user.email 'email')

	read -r -d '' QUERY <<-EOF
		SELECT
			job.id,
			job.create_time AT TIME ZONE 'UTC' as create_time,
			job.tool_id,
			job.tool_version,
			job.handler,
			job.destination_id,
			$email
		FROM
			job,
			galaxy_user
		WHERE
			job.create_time >= (now() AT TIME ZONE 'UTC' - '$hours hours'::interval) AND
			job.state = 'error' AND
			job.user_id = galaxy_user.id
		ORDER BY
			job.id
	EOF
}

query_server-users() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="WHERE create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="count=4"
	tags="active=0;external=1;deleted=2;purged=3"

	read -r -d '' QUERY <<-EOF
		SELECT
			active, external, deleted, purged, count(*) as count
		FROM
			galaxy_user
		$date_filter
		GROUP BY
			active, external, deleted, purged
	EOF
}

query_server-groups() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="WHERE create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi


	fields="count=1"
	tags="name=0"

	read -r -d '' QUERY <<-EOF
		SELECT
			galaxy_group.name, count(*)
		FROM
			galaxy_group, user_group_association
		WHERE
			user_group_association.group_id = galaxy_group.id
		GROUP BY name
	EOF
}

query_server-datasets() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="WHERE create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="count=4;size=5"
	tags="state=0;deleted=1;purged=2;object_store_id=3"

	read -r -d '' QUERY <<-EOF
		SELECT
			state, deleted, purged, object_store_id, count(*), coalesce(sum(total_size), 0)
		FROM
			dataset
		GROUP BY
			state, deleted, purged, object_store_id
	EOF
}

query_server-hda() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="AND history_dataset_association.create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="sum=2;avg=3;min=4;max=5"
	tags="extension=0;deleted=1"

	read -r -d '' QUERY <<-EOF
		SELECT
			history_dataset_association.extension,
			history_dataset_association.deleted,
			coalesce(sum(dataset.file_size), 0),
			coalesce(avg(dataset.file_size), 0)::bigint,
			coalesce(min(dataset.file_size), 0),
			coalesce(max(dataset.file_size), 0),
			count(*)
		FROM
			history_dataset_association, dataset
		WHERE
			history_dataset_association.dataset_id = dataset.id
			${date_filter}
		GROUP BY
			history_dataset_association.extension, history_dataset_association.deleted
	EOF
}

query_server-ts-repos() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="WHERE create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="count=2"
	tags="tool_shed=0;owner=1"

	read -r -d '' QUERY <<-EOF
		SELECT
			tool_shed, owner, count(*)
		FROM
			tool_shed_repository
		$date_filter
		GROUP BY
			tool_shed, owner
		ORDER BY count desc
	EOF
}

query_server-histories() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="WHERE create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="count=6"
	tags="deleted=0;purged=1;published=2;importable=3;importing=4;genome_build=5"

	read -r -d '' QUERY <<-EOF
		SELECT
			deleted, purged, published, importable, importing, genome_build, count(*)
		FROM history
		${date_filter}
		GROUP BY
			deleted, purged, published, importable, importing, genome_build
	EOF
}


query_server-jobs() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="WHERE create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="count=3"
	tags="state=0;job_runner_name=1;destination_id=2"

	read -r -d '' QUERY <<-EOF
		SELECT
			state, job_runner_name, destination_id, count(*)
		FROM
			job
		${date_filter}
		GROUP BY
			state, job_runner_name, destination_id
	EOF
}

query_server-allocated-cpu() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="AND job.create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="cpu_years=1"
	tags="job_runner_name=0"

	read -r -d '' QUERY <<-EOF
		SELECT
			job.job_runner_name,
			round(sum(a.metric_value * b.metric_value), 2) AS cpu_seconds
		FROM
			job_metric_numeric AS a,
			job_metric_numeric AS b,
			job
		WHERE
			b.job_id = a.job_id
			AND a.job_id = job.id
			AND a.metric_name = 'runtime_seconds'
			AND b.metric_name = 'galaxy_slots'
			${date_filter}
		GROUP BY
			job.job_runner_name
	EOF
}

query_server-workflows() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="WHERE create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="count=3"
	tags="deleted=0;importable=1;published=2"

	read -r -d '' QUERY <<-EOF
		SELECT
			deleted, importable, published, count(*)
		FROM
			stored_workflow
		${date_filter}
		GROUP BY
			deleted, importable, published
	EOF
}

query_server-workflow-invocations() {
	handle_help "$@" <<-EOF
	EOF

	date_filter=""
	if (( $# > 0 )); then
		date_filter="WHERE create_time AT TIME ZONE 'UTC' <= '$1'::date"
	fi

	fields="count=2"
	tags="scheduler=0;handler=1"

	read -r -d '' QUERY <<-EOF
		SELECT
			scheduler, handler, count(*)
		FROM
			workflow_invocation
		${date_filter}
		GROUP BY
			scheduler, handler
	EOF
}

query_workflow-invocation-status() { ## query workflow-invocation-status: Report on how many workflows are in new state by handler
	handle_help "$@" <<-EOF
		Really only intended to be used in influx queries.
	EOF

	fields="count=2"
	tags="scheduler=0;handler=1"

	read -r -d '' QUERY <<-EOF
		SELECT
			scheduler,
			handler,
			count(*)
		FROM
			workflow_invocation
		WHERE state = 'new'
		GROUP BY handler, scheduler
	EOF
}
filter_pg2md() { ## filter pg2md: Convert postgres table format outputs to something that can be pasted as markdown
	handle_help "$@" <<-EOF
		Imagine doing something like:

		    $ gxadmin query active-users 2018 | gxadmin filter pg2md
		    unique_users  |        month
		    ------------- | --------------------
		    811           | 2018-12-01 00:00:00
		    658           | 2018-11-01 00:00:00
		    583           | 2018-10-01 00:00:00
		    444           | 2018-09-01 00:00:00
		    342           | 2018-08-01 00:00:00
		    379           | 2018-07-01 00:00:00
		    370           | 2018-06-01 00:00:00
		    330           | 2018-05-01 00:00:00
		    274           | 2018-04-01 00:00:00
		    186           | 2018-03-01 00:00:00
		    168           | 2018-02-01 00:00:00
		    122           | 2018-01-01 00:00:00

		and it should produce a nicely formatted table
	EOF
	cat | sed 's/--+--/- | -/g;s/^\(\s\+\)\([^|]\+\) |/\2 \1|/g' | head -n -2
}

filter_identicon(){ ## filter identicon: Convert an input data stream into an identicon (e.g. with hostname)
	handle_help "$@" <<-EOF
		Given an input data stream, digest it, and colour it using the same logic as digest-color

		    $ echo test | ./gxadmin filter identicon
		      ██████
		    ██      ██
		    ██  ██  ██
		      ██████
		    ██  ██  ██

		(Imagine that it is a nice pink/blue colour scheme)
	EOF

	cat | python -c "$identicon_script"
}

filter_digest-color() { ## filter digest-color: Color an input stream based on the contents (e.g. hostname)
	handle_help "$@" <<-EOF
		Colors entire input stream based on digest of entire input's contents.
		Mostly useful for colouring a hostname or some similar value.

		    $ echo test | ./gxadmin filter digest-color
		    test

		(Imagine that it is light blue text on a pink background)

		**NOTE** If the output isn't coloured properly, try:

		    export TERM=screen-256color
	EOF

	data="$(cat)"
	fg_color=$((16#$(echo "$data" | perl -pe "chomp if eof" | sha256sum | cut -c1-2)))
	bg_color=$(($fg_color + 15))

	echo "$(tput setaf $fg_color)$(tput setab $bg_color)${data}$(tput sgr0)"
}

filter_hexdecode() { ## filter hexdecode: Decodes any hex blobs from postgres outputs
	handle_help "$@" <<-EOF
		This automatically replaces any hex strings (\\x[a-f0-9]+) with their decoded versions. This can allow you to query galaxy metadata, decode it, and start processing it with JQ. Just pipe your query to it and it will replace it wherever it is found.

		    [galaxy@sn04 ~]$ psql -c  'select metadata from history_dataset_association limit 10;'
		                                 metadata
		    ------------------------------------------------------------------------------------------------------------------
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a206e756c6c2c202264626b6579223a205b223f225d2c202273657175656e636573223a206e756c6c7d
		     \\x7b22646174615f6c696e6573223a20333239312c202264626b6579223a205b223f225d2c202273657175656e636573223a20317d
		     \\x7b22646174615f6c696e6573223a20312c202264626b6579223a205b223f225d7d
		     \\x7b22646174615f6c696e6573223a20312c202264626b6579223a205b223f225d7d
		     \\x7b22646174615f6c696e6573223a20312c202264626b6579223a205b223f225d7d
		     \\x7b22646174615f6c696e6573223a20312c202264626b6579223a205b223f225d7d
		    (10 rows)

		    [galaxy@sn04 ~]$ psql -c  'select metadata from history_dataset_association limit 10;'  | gxadmin filter hexdecode
		                                 metadata
		    ------------------------------------------------------------------------------------------------------------------
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": null, "dbkey": ["?"], "sequences": null}
		     {"data_lines": 3291, "dbkey": ["?"], "sequences": 1}
		     {"data_lines": 1, "dbkey": ["?"]}
		     {"data_lines": 1, "dbkey": ["?"]}
		     {"data_lines": 1, "dbkey": ["?"]}
		     {"data_lines": 1, "dbkey": ["?"]}
		    (10 rows)

		Or to query for the dbkeys uesd by datasets:

		    [galaxy@sn04 ~]$ psql -c  'copy (select metadata from history_dataset_association limit 1000) to stdout' | \\
		        gxadmin filter hexdecode | \\
		        jq -r '.dbkey[0]' 2>/dev/null | sort | uniq -c | sort -nr
		        768 ?
		        118 danRer7
		         18 hg19
		         17 mm10
		         13 mm9
		          4 dm3
		          1 TAIR10
		          1 hg38
		          1 ce10
	EOF

	cat | python -c "$hexdecodelines"
}
uwsgi_stats_influx() { ## uwsgi stats_influx: Deprecated, use uwsgi stats-influx
	uwsgi_stats-influx
}

uwsgi_stats-influx(){ ## uwsgi stats-influx <addr>: InfluxDB formatted output for the current stats
	handle_help "$@" <<-EOF
		Contact a specific uWSGI stats address (requires uwsgi binary on path)
		and requests the current stats + formats them for InfluxDB. For some
		reason it has trouble with localhost vs IP address, so recommend that
		you use IP.

		    $ gxadmin uwsgi stats-influx 127.0.0.1:9191
		    uwsgi.locks,addr=127.0.0.1:9191,group=user_0 count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=signal count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=filemon count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=timer count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=rbtimer count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=cron count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=thunder count=2006859
		    uwsgi.locks,addr=127.0.0.1:9191,group=rpc count=0
		    uwsgi.locks,addr=127.0.0.1:9191,group=snmp count=0
		    uwsgi.general,addr=127.0.0.1:9191 listen_queue=0,listen_queue_errors=0,load=0,signal_queue=0
		    uwsgi.sockets,addr=127.0.0.1:9191,name=127.0.0.1:4001,proto=uwsgi queue=0,max_queue=100,shared=0,can_offload=0
		    uwsgi.worker,addr=127.0.0.1:9191,id=1 accepting=1,requests=65312,exceptions=526,harakiri_count=26,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=17433008661,respawn_count=27,tx=15850829410,avg_rt=71724
		    uwsgi.worker,addr=127.0.0.1:9191,id=2 accepting=1,requests=67495,exceptions=472,harakiri_count=51,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=15467746010,respawn_count=52,tx=15830867066,avg_rt=65380
		    uwsgi.worker,addr=127.0.0.1:9191,id=3 accepting=1,requests=67270,exceptions=520,harakiri_count=35,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=14162158015,respawn_count=36,tx=15799661545,avg_rt=73366
		    uwsgi.worker,addr=127.0.0.1:9191,id=4 accepting=1,requests=66434,exceptions=540,harakiri_count=34,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=15740205807,respawn_count=35,tx=16231969649,avg_rt=75468
		    uwsgi.worker,addr=127.0.0.1:9191,id=5 accepting=1,requests=67021,exceptions=534,harakiri_count=38,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=14573155758,respawn_count=39,tx=16517287963,avg_rt=140855
		    uwsgi.worker,addr=127.0.0.1:9191,id=6 accepting=1,requests=66810,exceptions=483,harakiri_count=24,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=19107513635,respawn_count=25,tx=15945313469,avg_rt=64032
		    uwsgi.worker,addr=127.0.0.1:9191,id=7 accepting=1,requests=66544,exceptions=460,harakiri_count=35,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=14240478391,respawn_count=36,tx=15499531841,avg_rt=114981
		    uwsgi.worker,addr=127.0.0.1:9191,id=8 accepting=1,requests=67577,exceptions=517,harakiri_count=35,signals=0,signal_queue=0,status="idle",rss=0,vsz=0,running_time=14767971195,respawn_count=36,tx=15780639229,avg_rt=201275

		For multiple zerglings you can run this for each and just 2>/dev/null

		    PATH=/opt/galaxy/venv/bin:/sbin:/bin:/usr/sbin:/usr/bin gxadmin uwsgi stats-influx 127.0.0.1:9190 2>/dev/null
		    PATH=/opt/galaxy/venv/bin:/sbin:/bin:/usr/sbin:/usr/bin gxadmin uwsgi stats-influx 127.0.0.1:9191 2>/dev/null
		    exit 0

		And it will fetch only data for responding uwsgis.
	EOF
	address="$1"; shift

	# fetch data
	uwsgi=$(which uwsgi)
	data="$($uwsgi --connect-and-read $address 2>&1)"

	echo "$data" | \
		jq -r '.locks[] | to_entries[] |  "uwsgi.locks,addr='$address',group=\(.key) count=\(.value)"' | \
		sed 's/group=user 0/group=user_0/g'

	echo "$data" | \
		jq -r '. | "uwsgi.general,addr='$address' listen_queue=\(.listen_queue),listen_queue_errors=\(.listen_queue_errors),load=\(.load),signal_queue=\(.signal_queue)"'

	echo "$data" | \
		jq -r '.sockets[] | "uwsgi.sockets,addr='$address',name=\(.name),proto=\(.proto) queue=\(.queue),max_queue=\(.max_queue),shared=\(.shared),can_offload=\(.can_offload)"'

	echo "$data" | \
		jq -r '.workers[] | "uwsgi.worker,addr='$address',id=\(.id) accepting=\(.accepting),requests=\(.requests),exceptions=\(.exceptions),harakiri_count=\(.harakiri_count),signals=\(.signals),signal_queue=\(.signal_queue),status=\"\(.status)\",rss=\(.rss),vsz=\(.vsz),running_time=\(.running_time),respawn_count=\(.respawn_count),tx=\(.tx),avg_rt=\(.avg_rt)"' | \
		sed 's/"busy"/1/g;s/"idle"/0/g;'
}

uwsgi_status() { ## uwsgi status: Current system status
	handle_help "$@" <<-EOF
		Current status of all uwsgi processes
	EOF

	echo "galaxy-zergpool:   $(systemctl status galaxy-zergpool | grep Active:)"
	for i in {0..3}; do
		echo "galaxy-zergling@$i: $(systemctl status galaxy-zergling@$i | grep Active:)"
	done
	for i in {0..11}; do
		echo "galaxy-handler@$i:  $(systemctl status galaxy-handler@$i | grep Active:)"
	done
}

uwsgi_memory() { ## uwsgi memory: Current system memory usage
	handle_help "$@" <<-EOF
		Obtain memory usage of the various Galaxy processes

		Also consider using systemd-cgtop
	EOF

	echo "galaxy-zergpool.service $(cat /sys/fs/cgroup/memory/system.slice/galaxy-zergpool.service/memory.memsw.usage_in_bytes)"
	for folder in $(find /sys/fs/cgroup/memory/system.slice/system-galaxy\\x2dzergling.slice/ -mindepth 1 -type d -name 'galaxy-zergling*service'); do
		service=$(basename $folder)
		echo "$service $(cat $folder/memory.memsw.usage_in_bytes)"
	done

	for folder in $(find /sys/fs/cgroup/memory/system.slice/system-galaxy\\x2dhandler.slice/ -mindepth 1 -type d -name 'galaxy-handler*service'); do
		service=$(basename $folder)
		echo "$service $(cat $folder/memory.memsw.usage_in_bytes)"
	done
}

uwsgi_pids() { ## uwsgi pids: Galaxy process PIDs
	handle_help "$@" <<-EOF
		Obtain memory usage of the various Galaxy processes
	EOF

	echo "galaxy-zergpool:   $(systemctl status galaxy-zergpool   | grep PID: | cut -d' ' -f 4)"
	for i in {0..3}; do
		echo "galaxy-zergling@$i: $(systemctl status galaxy-zergling@$i | grep PID: | cut -d' ' -f 4)"
	done
	for i in {0..11}; do
		echo "galaxy-handler@$i:  $(systemctl status galaxy-handler@$i | grep PID: | cut -d' ' -f 4)"
	done
}

uwsgi_stats() { ## uwsgi stats: uwsgi stats
	uwsgi_stats-influx 127.0.0.1:4010 2>/dev/null || true
	uwsgi_stats-influx 127.0.0.1:4011 2>/dev/null || true
	uwsgi_stats-influx 127.0.0.1:4012 2>/dev/null || true
	uwsgi_stats-influx 127.0.0.1:4013 2>/dev/null || true


	echo "systemd service=galaxy-zergpool memory=$(cat /sys/fs/cgroup/memory/system.slice/galaxy-zergpool.service/memory.memsw.usage_in_bytes)"
	for folder in $(find /sys/fs/cgroup/memory/system.slice/system-galaxy\\x2dzergling.slice/ -mindepth 1 -type d -name 'galaxy-zergling*service'); do
		service=$(basename $folder)
		echo "systemd service=$service memory=$(cat $folder/memory.memsw.usage_in_bytes)"
	done

	for folder in $(find /sys/fs/cgroup/memory/system.slice/system-galaxy\\x2dhandler.slice/ -mindepth 1 -type d -name 'galaxy-handler*service'); do
		service=$(basename $folder)
		echo "systemd service=$service memory=$(cat $folder/memory.memsw.usage_in_bytes)"
	done
}

uwsgi_zerg-swap() { ## uwsgi zerg-swap: Swap zerglings in order (unintelligent version)
	handle_help "$@" <<-EOF
		This is the "dumb" version which loops across the zerglings and restarts them in series
	EOF

	# Obtain current running ones
	for zergling in $(status | grep -v inactive | grep zergling | cut -d: -f1); do
		systemctl restart $zergling
		number=$(echo "$zergling" | sed 's/.*@//g')
		stats="127.0.0.1:401$number"
		wait_for_url $stats
		success "$zergling is alive"
	done
}

uwsgi_zerg-scale-up() { ## uwsgi zerg-scale-up: Add another zergling to deal with high load
	handle_help "$@" <<-EOF
	EOF

	# Obtain current running ones
	an_inactive=$(status | grep inactive | grep zergling | head -n1 | cut -d: -f1)
	echo "$an_inactive is inactive, starting"
	systemctl start $an_inactive
}

uwsgi_zerg-scale-down() { ## uwsgi zerg-scale-down: Remove an extraneous zergling
	handle_help "$@" <<-EOF
	EOF

	# Obtain current running ones
	active="$(status | grep -v inactive | grep zergling | cut -d: -f1)"
	num_active=$(echo "$active" | wc -l)

	if (( $num_active <= 2 )); then
		error "We do not permit scaling below 2 zerglings"
		exit 1
	fi

	# Kill highest numbered one
	warning "$to_kill will be stopped"
	systemctl stop $to_kill
}

uwsgi_zerg-strace() { ## uwsgi zerg-strace [number]: Strace a zergling
	handle_help "$@" <<-EOF
	EOF

	if (( $# > 0 )); then
		number=$1;
	fi
	procs=$(uwsgi_pids | grep zergling@${number} | cut -d':' -f2 | sed 's/\s*//g;' | tr '\n' ' ' | sed 's/^\s*//;s/\s*$//g;s/ / -p /g')
	strace -e open,openat -p $procs
}

uwsgi_handler-strace() { ## uwsgi handler-strace [number]: Strace a handler
	handle_help "$@" <<-EOF
	EOF

	if (( $# > 0 )); then
		number=$1;
	fi
	procs=$(uwsgi_pids | grep handler@${number} | cut -d':' -f2 | sed 's/\s*//g;' | tr '\n' ' ' | sed 's/^\s*//;s/\s*$//g;s/ / -p /g')
	strace -e open,openat -p $procs
}

uwsgi_handler-restart() { ## uwsgi handler-restart: Restart all handlers
	handle_help "$@" <<-EOF
	EOF

	for i in {0..11}; do
		systemctl restart galaxy-handler@$i &
	done
}


uwsgi_lastlog(){ ## uwsgi lastlog: Fetch the number of seconds since the last log message was written
	handle_help "$@" <<-EOF
		Lets you know if any of your workers or handlers have maybe stopped processing jobs.

		    $ gxadmin uwsgi lastlog
		    journalctl.lastlog,service=galaxy-handler@0 seconds=8
		    journalctl.lastlog,service=galaxy-handler@1 seconds=2
		    journalctl.lastlog,service=galaxy-handler@2 seconds=186
		    journalctl.lastlog,service=galaxy-handler@3 seconds=19
		    journalctl.lastlog,service=galaxy-handler@4 seconds=6
		    journalctl.lastlog,service=galaxy-handler@5 seconds=80
		    journalctl.lastlog,service=galaxy-handler@6 seconds=52
		    journalctl.lastlog,service=galaxy-handler@7 seconds=1
		    journalctl.lastlog,service=galaxy-handler@8 seconds=79
		    journalctl.lastlog,service=galaxy-handler@9 seconds=40
		    journalctl.lastlog,service=galaxy-handler@10 seconds=123
		    journalctl.lastlog,service=galaxy-handler@11 seconds=13
		    journalctl.lastlog,service=galaxy-zergling@0 seconds=0
		    journalctl.lastlog,service=galaxy-zergling@1 seconds=0
		    journalctl.lastlog,service=galaxy-zergling@2 seconds=2866

	EOF

	NOW=$(date +%s)

	for i in {0..11}; do
		lines=$(journalctl -u galaxy-handler@$i -n 1 --no-pager)
		if (( $(echo "$lines" | wc -l) > 1 )); then
			timestamp=$(journalctl -u galaxy-handler@$i -n 1 --no-pager | grep -v 'Logs begin' | awk '{print $1" "$2" "$3}');
			unix=$(date -d "$timestamp" +%s)
			date_diff=$(($NOW - $unix));
			echo "journalctl.lastlog,service=galaxy-handler@$i seconds=$date_diff";
		fi
	done

	for i in {0..4}; do
		lines=$(journalctl -u galaxy-zergling@$i -n 1 --no-pager)
		if (( $(echo "$lines" | wc -l) > 1 )); then
			timestamp=$(journalctl -u galaxy-zergling@$i -n 1 --no-pager | grep -v 'Logs begin' | awk '{print $1" "$2" "$3}');
			unix=$(date -d "$timestamp" +%s)
			date_diff=$(($NOW - $unix));
			echo "journalctl.lastlog,service=galaxy-zergling@$i seconds=$date_diff";
		fi
	done
}
galaxy_cleanup() { ## galaxy cleanup [days]: Cleanup histories/hdas/etc for past N days (default=30)
	handle_help "$@" <<-EOF
		Cleanup histories/hdas/etc for past N days using the python objects-based method
	EOF

	days=30
	if (( $# > 0 )); then
		days=$1
	fi

	assert_set_env GALAXY_ROOT
	assert_set_env GALAXY_CONFIG_FILE
	assert_set_env GALAXY_LOG_DIR

	run_date=$(date --rfc-3339=seconds)

	for action in {delete_userless_histories,delete_exported_histories,purge_deleted_histories,purge_deleted_hdas,delete_datasets,purge_datasets}; do
		python $GALAXY_ROOT/scripts/cleanup_datasets/pgcleanup.py \
			-c $GALAXY_CONFIG_FILE \
			-o $days \
			-l $GALAXY_LOG_DIR \
			-s $action \
			-w 128MB \
			 >> "$GALAXY_LOG_DIR/cleanup-${run_date}-${action}.log" \
			2>> "$GALAXY_LOG_DIR/cleanup-${run_date}-${action}.err";

		# Something that telegraf can consume
		ec=$?
		if (( ec == 0 )); then
			echo "cleanup_datasets,group=$action success=1"
		else
			echo "cleanup_datasets,group=$action success=0"
		fi
	done
}

galaxy_migrate-tool-install-to-sqlite() { ## galaxy migrate-tool-install-to-sqlite: Converts normal potsgres toolshed repository tables into the SQLite version
	handle_help "$@" <<-EOF
		    $ gxadmin migrate-tool-install-to-sqlite
		    Creating new sqlite database: galaxy_install.sqlite
		    Migrating tables
		      export: tool_shed_repository
		      import: tool_shed_repository
		      ...
		      export: repository_repository_dependency_association
		      import: repository_repository_dependency_association
		    Complete
	EOF

	# Export tables
	if [[ -f  galaxy_install.sqlite ]]; then
		error "galaxy_install.sqlite exists, not overwriting"
		exit 1
	fi

	success "Creating new sqlite database: galaxy_install.sqlite"
	empty_schema=$(mktemp)
	echo "
	PRAGMA foreign_keys=OFF;
	BEGIN TRANSACTION;
	CREATE TABLE migrate_version (
		repository_id VARCHAR(250) NOT NULL,
		repository_path TEXT,
		version INTEGER,
		PRIMARY KEY (repository_id)
	);
	CREATE TABLE tool_shed_repository (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed VARCHAR(255),
		name VARCHAR(255),
		description TEXT,
		owner VARCHAR(255),
		changeset_revision VARCHAR(255),
		deleted BOOLEAN,
		metadata BLOB,
		includes_datatypes BOOLEAN,
		installed_changeset_revision VARCHAR(255),
		uninstalled BOOLEAN,
		dist_to_shed BOOLEAN,
		ctx_rev VARCHAR(10),
		status VARCHAR(255),
		error_message TEXT,
		tool_shed_status BLOB,
		PRIMARY KEY (id),
		CHECK (deleted IN (0, 1))
	);
	CREATE TABLE tool_version (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_id VARCHAR(255),
		tool_shed_repository_id INTEGER,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE tool_version_association (
		id INTEGER NOT NULL,
		tool_id INTEGER NOT NULL,
		parent_id INTEGER NOT NULL,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_id) REFERENCES tool_version (id),
		FOREIGN KEY(parent_id) REFERENCES tool_version (id)
	);
	CREATE TABLE migrate_tools (
		repository_id VARCHAR(255),
		repository_path TEXT,
		version INTEGER
	);
	CREATE TABLE tool_dependency (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER NOT NULL,
		name VARCHAR(255),
		version VARCHAR(40),
		type VARCHAR(40),
		status VARCHAR(255),
		error_message TEXT,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE repository_dependency (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER NOT NULL,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id)
	);
	CREATE TABLE repository_repository_dependency_association (
		id INTEGER NOT NULL,
		create_time DATETIME,
		update_time DATETIME,
		tool_shed_repository_id INTEGER,
		repository_dependency_id INTEGER,
		PRIMARY KEY (id),
		FOREIGN KEY(tool_shed_repository_id) REFERENCES tool_shed_repository (id),
		FOREIGN KEY(repository_dependency_id) REFERENCES repository_dependency (id)
	);
	CREATE INDEX ix_tool_shed_repository_name ON tool_shed_repository (name);
	CREATE INDEX ix_tool_shed_repository_deleted ON tool_shed_repository (deleted);
	CREATE INDEX ix_tool_shed_repository_tool_shed ON tool_shed_repository (tool_shed);
	CREATE INDEX ix_tool_shed_repository_changeset_revision ON tool_shed_repository (changeset_revision);
	CREATE INDEX ix_tool_shed_repository_owner ON tool_shed_repository (owner);
	CREATE INDEX ix_tool_shed_repository_includes_datatypes ON tool_shed_repository (includes_datatypes);
	CREATE INDEX ix_tool_version_tool_shed_repository_id ON tool_version (tool_shed_repository_id);
	CREATE INDEX ix_tool_version_association_tool_id ON tool_version_association (tool_id);
	CREATE INDEX ix_tool_version_association_parent_id ON tool_version_association (parent_id);
	CREATE INDEX ix_tool_dependency_tool_shed_repository_id ON tool_dependency (tool_shed_repository_id);
	CREATE INDEX ix_repository_dependency_tool_shed_repository_id ON repository_dependency (tool_shed_repository_id);
	CREATE INDEX ix_repository_repository_dependency_association_tool_shed_repository_id ON repository_repository_dependency_association (tool_shed_repository_id);
	CREATE INDEX ix_repository_repository_dependency_association_repository_dependency_id ON repository_repository_dependency_association (repository_dependency_id);
	COMMIT;
	" > ${empty_schema}
	sqlite3 galaxy_install.sqlite < ${empty_schema}
	rm ${empty_schema}

	success "Migrating tables"

	# tool_shed_repository is special :(
	table=tool_shed_repository
	success "  export: ${table}"
	export_csv=$(mktemp /tmp/tmp.gxadmin.${table}.XXXXXXXXXXX)
	psql -c "COPY (select
		id, create_time, update_time, tool_shed, name, description, owner, changeset_revision, case when deleted then 1 else 0 end, metadata, includes_datatypes, installed_changeset_revision, uninstalled, dist_to_shed, ctx_rev, status, error_message, tool_shed_status from $table) to STDOUT with CSV" > $export_csv;

	success "  import: ${table}"
	echo ".mode csv
.import ${export_csv} ${table}" | sqlite3 galaxy_install.sqlite
	if (( $? == 0 )); then
		rm ${export_csv}
	else
		error "  sql: ${export_csv}"
	fi

	sqlite3 galaxy_install.sqlite "insert into migrate_version values ('ToolShedInstall', 'lib/galaxy/model/tool_shed_install/migrate', 17)"
	# the rest are sane!
	for table in {tool_version,tool_version_association,migrate_tools,tool_dependency,repository_dependency,repository_repository_dependency_association}; do
		success "  export: ${table}"
		export_csv=$(mktemp /tmp/tmp.gxadmin.${table}.XXXXXXXXXXX)
		psql -c "COPY (select * from $table) to STDOUT with CSV" > $export_csv;

		success "  import: ${table}"
		echo ".mode csv
.import ${export_csv} ${table}" | sqlite3 galaxy_install.sqlite
		if (( $? == 0 )); then
			rm ${export_csv}
		else
			error "  sql: ${export_csv}"
		fi
	done

	success "Complete"
}
report_user-info() { ## report user-info <user_id|username|email>: Quick overview of a Galaxy user in your system
	handle_help "$@" <<-EOF
		This command lets you quickly find out information about a user. The output is formatted as markdown by default.

		    $ gxadmin report user-info helena-rasche
		    # Galaxy User 580

		      Property | Value
		    ---------- | -----
		            ID | helena-rasche (id=580) hxr@informatik.uni-freiburg.de
		       Created | 2017-07-26 14:47:37.575484
		    Properties | ext=f deleted=f purged=f active=t
		    Disk Usage | 137 GB

		    ## Groups/Roles

		    Groups: training-freiburg-rnaseq-2018, training-emc2018
		    Roles: admin, Backofen

		    ## Recent Jobs

		    Tool ID                      | Status | Created                    | Exit Code | Runtime
		    ----                         | ----   | ----                       | ---       | ----
		    Grep1                        | ok     | 2019-01-21 07:27:24.472706 | 0         | 00:01:19
		    CONVERTER_fasta_to_tabular   | ok     | 2019-01-21 07:27:24.339862 | 0         | 00:03:34
		    secure_hash_message_digest   | ok     | 2019-01-18 16:43:44.262265 | 0         | 00:00:08
		    CONVERTER_gz_to_uncompressed | ok     | 2019-01-18 10:18:23.99171  | 0         | 00:10:02
		    upload1                      | ok     | 2019-01-18 08:44:24.955622 | 0         | 01:11:07
		    echo_main_env                | ok     | 2019-01-17 16:45:04.019233 | 0         | 00:00:29
		    secure_hash_message_digest   | ok     | 2019-01-17 16:03:21.33665  | 0         | 00:00:07
		    secure_hash_message_digest   | ok     | 2019-01-17 16:03:20.937433 | 0         | 00:00:09

		    ## Largest Histories

		    History ID | Name                         | Size
		    ----       | ----                         | ----
		    20467      | imported: RNASEQ             | 52 GB
		    94748      | imported: ChIP-seq           | 49 GB
		    94761      | reduced history-export problem |   49 GB
		    102448     | Wheat Genome                 | 42 GB
		    38923      | imported: Zooplankton        | 29 GB
		    64974      | imported: 65991-A            | 17 GB
		    20488      | Unnamed history              | 15 GB
		    19414      | Unnamed history              | 12 GB
		    92407      | Testing                      | 11 GB
		    60522      | example1/wf3-shed-tools.ga   | 5923 MB
	EOF

	# Metada
	read -r -d '' qstr <<-EOF
		SELECT
			username, id, email, create_time AT TIME ZONE 'UTC' as create_time, external, deleted, purged, active, pg_size_pretty(disk_usage)
		FROM
			galaxy_user
		WHERE
			(galaxy_user.email = '$1' or galaxy_user.username = '$1' or galaxy_user.id = CAST(REGEXP_REPLACE(COALESCE('$1','0'), '[^0-9]+', '0', 'g') AS INTEGER))
	EOF
	results=$(query_tsv "$qstr")
	user_id=$(echo "$results" | awk '{print $2}')

	if [[ -z "$user_id" ]]; then
		error "Unknown user"
		exit 1
	fi

	# Groups
	read -r -d '' qstr <<-EOF
		select string_agg(galaxy_group.name, ', ') from user_group_association, galaxy_group where user_id = $user_id and user_group_association.group_id = galaxy_group.id
	EOF
	group_membership=$(query_tsv "$qstr")

	read -r -d '' qstr <<-EOF
		select string_agg(role.name, ', ') from user_role_association, role where user_id = $user_id and user_role_association.role_id = role.id and role.type not in ('private', 'sharing')
	EOF
	role_membership=$(query_tsv "$qstr")

	# Recent jobs
	read -r -d '' qstr <<-EOF
		SELECT
			tool_id, state, create_time AT TIME ZONE 'UTC' as create_time, exit_code, metric_value::text::interval
		FROM
			job, job_metric_numeric
		WHERE
			job.user_id = $user_id and job.id = job_metric_numeric.job_id and metric_name = 'runtime_seconds' order by job.id desc limit 10
	EOF
	recent_jobs=$(query_tsv "$qstr")
	recent_jobs2=$(printf "Tool ID\tStatus\tCreated\tExit Code\tRuntime\n----\t----\t----\t---\t----\n%s" "$recent_jobs" | sed 's/\t/\t | \t/g' | column -t -s'	')

	# Recent jobs
	read -r -d '' qstr <<-EOF
		SELECT
			tool_id, tool_version, handler, destination_id, state, create_time AT TIME ZONE 'UTC' as create_time, now() AT TIME ZONE 'UTC' - create_time  AT TIME ZONE 'UTC' as runtime
		FROM
			job
		WHERE
			job.user_id = $user_id and job.state in ('running', 'queued', 'new') order by job.id desc
	EOF
	running_jobs=$(query_tsv "$qstr")
	running_jobs2=$(printf "Tool ID\tTool Version\tHandler\tDestination\tState\tCreated\tRuntime\n----\t----\t----\t----\t----\t---\t----\n%s" "$running_jobs" | sed 's/\t/\t | \t/g' | column -t -s'	')

	# Recent workflows
	read -r -d '' qstr <<-EOF
		SELECT
			workflow_invocation.create_time,
			state,
			scheduler,
			handler,
			substring(workflow.name, 0, 20) AS workflow,
			substring(history.name, 0, 20) AS history
		FROM
			workflow_invocation
			JOIN workflow ON workflow_invocation.workflow_id = workflow.id
			JOIN history ON workflow_invocation.history_id = history.id
		WHERE
			history.user_id = $user_id
		ORDER BY
			create_time DESC
		LIMIT
			5
	EOF
	recent_wf=$(query_tsv "$qstr")
	recent_wf=$(printf "Created\tState\tScheduler\tHandler\tWorkflow\tHistory\n----\t----\t----\t----\t----\t----\n%s" "$recent_wf" | sed 's/\t/\t | \t/g' | column -t -s'	')

	# Largest Histories
	read -r -d '' qstr <<-EOF
		SELECT
			history.id, history.name, pg_size_pretty(sum(COALESCE(dataset.total_size, 0))) AS size, history.deleted, history.purged
		FROM
			history
			LEFT JOIN history_dataset_association ON history.id = history_dataset_association.history_id
			LEFT JOIN dataset ON history_dataset_association.dataset_id = dataset.id
		WHERE
			history.user_id = $user_id
		GROUP BY
			history.id, history.name
		ORDER BY
			sum(COALESCE(dataset.total_size, 0)) DESC
		LIMIT
			10
	EOF
	largest_histories=$(query_tsv "$qstr")
	largest_histories=$(printf "History ID\tName\tSize\tDeleted\tPurged\n----\t----\t----\t----\t----\n%s" "$largest_histories" | sed 's/\t/\t | \t/g' | column -t -s'	')

	read -r -d '' template <<EOF
# Galaxy User $user_id

  Property | Value
---------- | -----
        ID | %s (id=%s) %s
   Created | %s %s
Properties | ext=%s deleted=%s purged=%s active=%s
Disk Usage | %s %s

## Groups/Roles

Groups: %s
Roles: %s

## Recent Jobs

%s

## Running Jobs

%s

## Recent Workflow Invocations

%s

## Largest Histories

%s
\n
EOF
	printf "$template" $results "$group_membership" "$role_membership" "$recent_jobs2" "$running_jobs2" "$recent_wf" "$largest_histories"
}

report_job-info() { ## report job-info <id>: Information about a specific job
	handle_help "$@" <<-EOF
		    $ gxadmin report job-info 1
		     tool_id | state | username |        create_time         | job_runner_name | job_runner_external_id
		    ---------+-------+----------+----------------------------+-----------------+------------------------
		     upload1 | ok    | admin    | 2012-12-06 16:34:27.492711 | local:///       | 9347
	EOF

	assert_count $# 1 "Missing Job ID"
	username=$(gdpr_safe galaxy_user.username username)
	job_id=$1

	###       ###
	#  JOB INFO #
	###       ###

	read -r -d '' qstr <<-EOF
		SELECT
			job.tool_id,
			job.state,
			job.handler,
			job.create_time AT TIME ZONE 'UTC' as create_time,
			(now() AT TIME ZONE 'UTC' - job.create_time)::interval,
			COALESCE(job.job_runner_name, '?'),
			COALESCE(job.job_runner_external_id, '?')
		FROM job
		WHERE job.id = $job_id
	EOF
	results=$(query_tsv "$qstr")
	read -r -d '' template <<EOF
# Galaxy Job $job_id

Property      | Value
------------- | -----
         Tool | %s
        State | %s
      Handler | %s
      Created | %s %s (%s %s %s ago)
Job Runner/ID | %s / %s
EOF
	printf "$template"  $results

	###       ###
	# USER INFO #
	###       ###

	read -r -d '' qstr <<-EOF
		SELECT
			$username,
			job.user_id
		FROM job, galaxy_user
		WHERE job.id = $job_id AND job.user_id = galaxy_user.id
	EOF
	job_owner=$(query_tsv "$qstr")
	printf "\n        Owner | %s (id=%s)\n\n" $job_owner


	###         ###
	# DEST PARAMS #
	###         ###
	read -r -d '' qstr <<-EOF
		SELECT destination_params
		FROM job
		WHERE job.id = $job_id
	EOF
	results=$(query_tsv "$qstr")
	printf "## Destination Parameters\n\n"
	tbl=$(echo "$results" | python -c "$hexdecodelines" | jq -S '. | to_entries[] | [.key, .value] | @tsv' -r | sed 's/\t/\t`/g;s/$/`/g')
	printf "Key\tValue\n---\t---\n$tbl" | sed 's/\t/\t | \t/g' | column -t -s'	'

	###          ###
	# DEPENDENCIES #
	###          ###
	read -r -d '' qstr <<-EOF
		SELECT dependencies
		FROM job
		WHERE job.id = $job_id
	EOF
	results=$(query_tsv "$qstr")
	printf "\n## Dependencies\n\n"
	tbl=$(echo "$results" | python -c "$hexdecodelines" | jq -S '.[] | [.name, .version, .dependency_type, .cacheable, .exact, .environment_path, .model_class] | @tsv' -r)
	printf "Name\tVersion\tDependency Type\tCacheable\tExact\tEnvironment Path\tModel Class\n----\t-------\t---------------\t---------\t-----\t----------------\t-----------\n$tbl\n" | sed 's/\t/\t | \t/g'       #| column -t -s'	'

    ###        ###
    # JOB PARAMS #
    ###        ###
    read -r -d '' qstr <<-EOF
        SELECT j.name, j.value
        FROM job_parameter j
        WHERE j.job_id = $job_id
    EOF
    results=$(query_tsv "$qstr")
    printf "\n## Tool Parameters\n\n"
    printf "Name\tSettings\n---------\t------------------------------------\n$results\n\n" | sed 's/\t/\t | \t/g' | sed 's/[\"\`]//g'

	###      ###
	#  INPUTS  #
	###      ###
	read -r -d '' qstr <<-EOF
	SELECT
		jtod.job_id,
	    hda.name,
	    hda.extension,
	    hda.id,
	    hda.state,
	    hda.deleted,
	    hda.purged,
	    ds.id,
	    ds.state,
	    ds.deleted,
	    ds.purged,
	    pg_size_pretty(ds.total_size)
	FROM
	    job_to_input_dataset AS jtid,
		history_dataset_association AS hda,
		dataset AS ds,
		job_to_output_dataset as jtod
	WHERE
	    jtid.job_id = $1
		AND jtid.dataset_id = hda.id
		AND hda.dataset_id = ds.id
		AND jtod.dataset_id = ds.id
	EOF
	input_ds=$(query_tsv "$qstr")
	input_ds_tbl=$(printf "Job ID\tName\tExtension\thda-id\thda-state\thda-deleted\thda-purged\tds-id\tds-state\tds-deleted\tds-purged\tSize\n----\t----\t----\t----\t----\t----\t----\t----\t----\t----\t----\t----\n%s" "$input_ds" | sed 's/\t/\t | \t/g' )      #| column -t -s'	')

	###       ###
	#  OUTPUTS  #
	###       ###
	read -r -d '' qstr <<-EOF
	SELECT
	    hda.name,
	    hda.extension,
	    hda.id,
	    hda.state,
	    hda.deleted,
	    hda.purged,
	    ds.id,
	    ds.state,
	    ds.deleted,
	    ds.purged,
	    pg_size_pretty(ds.total_size)
	FROM
	    job_to_output_dataset AS jtod,
		history_dataset_association AS hda,
		dataset AS ds
	WHERE
	    jtod.job_id = $1
		AND jtod.dataset_id = hda.id
		AND hda.dataset_id = ds.id
	EOF
	output_ds=$(query_tsv "$qstr")
	output_ds_tbl=$(printf "Name\tExtension\thda-id\thda-state\thda-deleted\thda-purged\tds-id\tds-state\tds-deleted\tds-purged\tSize\n----\t----\t----\t----\t----\t----\t----\t----\t----\t----\t----\n%s" "$output_ds" | sed 's/\t/\t | \t/g' )          #| column -t -s'	')


	read -r -d '' template3 <<EOF

## Inputs

%s

## Outputs

%s

\n
EOF
	printf "$template3" "$input_ds_tbl" "$output_ds_tbl"
}


report_assigned-to-handler() { ## report assigned-to-handler <handler>: Report what items are assigned to a handler currently.
	handle_help "$@" <<-EOF
	EOF

	assert_count $# 1 "Missing Handler ID"
	printf "# Handler $1\n\n"

	# Jerbs
	printf "## Jobs\n\n"
	read -r -d '' qstr <<-EOF
		SELECT
			id, create_time, tool_id, state, object_store_id, user_id
		FROM job
		WHERE handler = '$1' and state in ('new', 'queued', 'running')
		ORDER BY create_time DESC
	EOF
	output_ds=$(query_tsv "$qstr")
	printf "ID\tCreate Time\tTool ID\tState\tObject Store\tUser ID\n----\t----\t----\t----\t----\t----\n%s" "$output_ds" | sed 's/\t/\t | \t/g' | column -t -s'	'


	# Workflows
	printf "## Workflows\n\n"
	read -r -d '' qstr <<-EOF
		SELECT
			id, create_time, workflow_id, history_id, state, scheduler, uuid
		FROM workflow_invocation
		WHERE handler = '$1' and state = 'new'
	EOF
	output_ds=$(query_tsv "$qstr")
	printf "ID\tCreate Time\tWorkflow ID\tHistory ID\tState\tScheduler\tUUID\n----\t----\t----\t----\t----\t----\t----\n%s" "$output_ds" | sed 's/\t/\t | \t/g' | column -t -s'	'
}
# Some unlisted functions that are probably not useful as things people might
# want to query (and thus have cluttering their query menu), but should be
# shared because that's the purpose of gxadmin, sharing all of these weird
# queries that we have :)

query_memory-and-cpu-on-same-node() {
	handle_help "$@" <<-EOF
		Helena needed to check the reported values of memory/cpu over a series
		of jobs on a specific host, to debug what went wrong with the cgroup
		metrics.

		TODO(hxr): find some way to note "unlisted" functions and let people do the help for them?
	EOF

	assert_count $# 1 "Missing host name"

	read -r -d '' QUERY <<-EOF
		SELECT
			jmn_a.metric_value AS "memory.memsw.max_usage_in_bytes",
			jmn_b.metric_value AS "cpuacct.usage",
			job.update_time
		FROM
			job_metric_numeric AS jmn_a, job_metric_numeric AS jmn_b, job
		WHERE
			jmn_a.job_id
			IN (
					SELECT
						job_id
					FROM
						job_metric_text
					WHERE
						metric_value = '$host'
					ORDER BY
						job_id DESC
					LIMIT
						200
				)
			AND jmn_a.metric_name = 'memory.memsw.max_usage_in_bytes'
			AND jmn_b.job_id
				IN (
						SELECT
							job_id
						FROM
							job_metric_text
						WHERE
							metric_value = '$host'
						ORDER BY
							job_id DESC
						LIMIT
							200
					)
			AND jmn_b.metric_name = 'cpuacct.usage'
			AND jmn_a.job_id = job.id
			AND jmn_b.job_id = job.id
		ORDER BY
			job.create_time DESC
	EOF
}

meta_update() { ## meta update: Update the script
	handle_help "$@" <<-EOF
	EOF

	tmp=$(mktemp);
	curl https://raw.githubusercontent.com/usegalaxy-eu/gxadmin/master/gxadmin > $tmp;
	chmod ugo+rx $tmp;
	mv $tmp $0;
	exit 0;
}

meta_cmdlist() {
	handle_help "$@" <<-EOF
	EOF

	IFS=$'\n'
	# TOC
	echo "## Commands"
	echo
	for section in $(grep -o '{ ## .*' $0 | grep -v grep | grep -v '| sed' | awk '{print $3}' | sort -u); do
		echo "# $section"            > docs/README.${section}.md
		echo                         >> docs/README.${section}.md
		echo "Command | Description" >> docs/README.${section}.md
		echo "------- | -----------" >> docs/README.${section}.md
		for command in $(grep -o '{ ## .*' $0 | grep -v grep | grep -v '| sed' | sort | sed 's/^{ ## //g' | grep "^$section"); do
			cmd_part="$(echo $command | sed 's/:.*//g;s/\s*<.*//g;s/\s*\[.*//')"
			desc_part="$(echo $command | sed 's/^[^:]*:\s*//g')"

			if [[ "$command" != *"Deprecated"* ]]; then
				# Main ToC
				echo "[\`${cmd_part}\`](#${key_part}) | $desc_part" >> docs/README.${section}.md
			else
				echo "\`${cmd_part}\` | $desc_part" >> docs/README.${section}.md
			fi
		done

		echo "### $section"
		echo
		echo "Command | Description"
		echo "------- | -----------"
		for command in $(grep -o '{ ## .*' $0 | grep -v grep | grep -v '| sed' | sort | sed 's/^{ ## //g' | grep "^$section"); do
			cmd_part="$(echo $command | sed 's/:.*//g;s/\s*<.*//g;s/\s*\[.*//')"
			desc_part="$(echo $command | sed 's/^[^:]*:\s*//g')"
			key_part="$(echo $cmd_part | sed 's/ /-/g')"

			if [[ "$command" != *"Deprecated"* ]]; then
				# Main ToC
				echo "[\`${cmd_part}\`](docs/README.${section}.md#${key_part}) | $desc_part"

				# Subsec documentation
				echo                          >> docs/README.${section}.md
				echo "### $cmd_part"          >> docs/README.${section}.md
				echo                          >> docs/README.${section}.md
				bash -c "$0 $cmd_part --help" >> docs/README.${section}.md
			else
				echo "\`${cmd_part}\` | $desc_part"
			fi
		done

		echo
	done
}

meta_slurp-current() { ## meta slurp-current: Executes what used to be "Galaxy Slurp"
	handle_help "$@" <<-EOF
		Obtain influx compatible metrics regarding the current state of the
		server. UseGalaxy.EU uses this to display things like "Current user
		count" and "Current dataset size/distribution/etc."

		It is expected that you are passing this straight to telegraf, there is
		no non-influx output supported currently.

		You can add your own functions which are included in this output, using
		the \$GXADMIN_SITE_SPECIFIC file. They must start with the prefix
		"query_server-", e.g. "query_server-mymetric".

		    $ gxadmin meta slurp-current
		    server-allocated-cpu,job_runner_name=condor cpu_years=102.00
		    server-allocated-cpu,job_runner_name=local cpu_years=1346.00
		    server-datasets,deleted=f,object_store_id=,state=error,purged=f count=37,size=29895528
		    server-datasets,deleted=f,object_store_id=,state=ok,purged=f count=72,size=76739510
		    server-datasets,deleted=f,object_store_id=,state=discarded,purged=f count=2,size=0
		    server-hda,deleted=f,extension=gff3 max=2682565,sum=2682565,avg=2682565,min=2682565
		    server-hda,deleted=t,extension=tabular max=468549,sum=597843,avg=22142,min=2
		    server-hda,deleted=f,extension=fastqsanger max=3,sum=3,avg=3,min=3
		    server-hda,deleted=f,extension=tabular max=2819293,sum=3270268,avg=155727,min=3
		    server-hda,deleted=f,extension=png max=800459,sum=7047695,avg=503407,min=142863
		    server-hda,deleted=t,extension=auto max=9571,sum=9571,avg=9571,min=9571
		    server-hda,deleted=t,extension=data max=2,sum=2,avg=2,min=2
		    server-hda,deleted=t,extension=txt max=5023,sum=5039,avg=2520,min=16
		    server-hda,deleted=t,extension=bigwig max=2972569,sum=5857063,avg=1464266,min=0
		    server-hda,deleted=t,extension=tar.gz max=209034,sum=1318690,avg=188384,min=91580
		    server-hda,deleted=f,extension=vcf_bgzip max=3965889,sum=3965889,avg=3965889,min=3965889
		    server-hda,deleted=t,extension=png max=2969711,sum=6584812,avg=1097469,min=183
		    server-hda,deleted=f,extension=txt max=9584828,sum=132124407,avg=4556014,min=0
		    server-hda,deleted=f,extension=bigwig max=14722,sum=17407,avg=8704,min=2685
		    server-hda,deleted=f,extension=tar.gz max=209025,sum=390421,avg=195211,min=181396
		    server-histories,importable=f,deleted=f,genome_build=?,published=f,importing=f,purged=f count=14
		    server-jobs,state=ok,destination_id=slurm_singularity,job_runner_name=local count=26
		    server-jobs,state=error,destination_id=condor_resub,job_runner_name=condor count=1
		    server-jobs,state=deleted,destination_id=local,job_runner_name=local count=1
		    server-jobs,state=error,destination_id=condor,job_runner_name=condor count=8
		    server-jobs,state=ok,destination_id=local,job_runner_name=local count=13
		    server-jobs,state=ok,destination_id=condor,job_runner_name=condor count=2
		    server-jobs,state=error,destination_id=local,job_runner_name=local count=3
		    server-users,active=t,deleted=f,external=f,purged=f count=3
		    server-workflows,deleted=f,importable=f,published=f count=3
	EOF

	for func in $(grep -s -h -o '^query_server-[a-z-]*' $0 $GXADMIN_SITE_SPECIFIC | sort | sed 's/query_//g'); do
		obtain_query $func
		query_influx "$QUERY" "$query_name" "$fields" "$tags"
	done
}

meta_slurp-upto() { ## meta slurp-upto [yyyy-mm-dd]: Slurps data "up to" a specific date.
	handle_help "$@" <<-EOF
		Obtain influx compatible metrics regarding the summed state of the
		server up to a specific date. UseGalaxy.EU uses this to display things
		like charts of "how many users were registered as of date X". You can
		backfill data from your server by running a for loop like:

		    #!/bin/bash
		    for i in range {1..365}; do
		        gxadmin meta slurp-upto \$(date -d "$i days ago" "+%Y-%m-%d") | <get into influx somehow>
		    done

		It is expected that you are passing this straight to telegraf, there is
		no non-influx output supported currently.

		This calls all of the same functions as 'gxadmin meta slurp-current',
		but with date filters for the entries' creation times.

		You can add your own functions which are included in this output, using
		the \$GXADMIN_SITE_SPECIFIC file. They must start with the prefix
		"query_server-", e.g. "query_server-mymetric". They should include a
		date filter as well, or the metrics reported here will be less useful.

		    $ gxadmin meta slurp-upto 2019-01-01
		    server-allocated-cpu.daily,job_runner_name=condor cpu_years=102.00
		    server-datasets.daily,deleted=f,object_store_id=,state=error,purged=f count=37,size=29895528
		    server-datasets.daily,deleted=f,object_store_id=,state=ok,purged=f count=72,size=76739510
		    server-datasets.daily,deleted=f,object_store_id=,state=discarded,purged=f count=2,size=0
		    server-hda.daily,deleted=t,extension=data max=2,sum=2,avg=2,min=2
		    server-hda.daily,deleted=t,extension=txt max=5023,sum=5039,avg=2520,min=16
		    server-hda.daily,deleted=f,extension=fastqsanger max=3,sum=3,avg=3,min=3
		    server-hda.daily,deleted=f,extension=tabular max=3,sum=51,avg=3,min=3
		    server-hda.daily,deleted=t,extension=tabular max=468549,sum=552788,avg=21261,min=2
		    server-histories.daily,importable=f,deleted=f,genome_build=?,published=f,importing=f,purged=f count=5
		    server-jobs.daily,state=error,destination_id=condor,job_runner_name=condor count=8
		    server-jobs.daily,state=error,destination_id=condor_resub,job_runner_name=condor count=1
		    server-jobs.daily,state=error,destination_id=condor_a,job_runner_name=condor count=11
		    server-jobs.daily,state=ok,destination_id=condor,job_runner_name=condor count=2
		    server-jobs.daily,state=ok,destination_id=condor_a,job_runner_name=condor count=23
		    server-users.daily,active=t,deleted=f,external=f,purged=f count=1
		    server-workflows.daily,deleted=f,importable=f,published=f count=1

	EOF

	for func in $(grep -s -h -o '^query_server-[a-z-]*' $0 $GXADMIN_SITE_SPECIFIC | sort | sed 's/query_//g'); do
		obtain_query $func $1
		query_influx "$QUERY" "$query_name.daily" "$fields" "$tags"
	done
}

meta_error() {
	error "$@"
}

meta_warning() {
	warning "$@"
}

meta_success() {
	success "$@"
}
meta_completion() {
	IFS=$'\n'
	commands="$(grep -o '{ ## .*' $0 | grep -v grep | grep -v '| sed' | sort | sed 's/^{ ## //g' | \
		sed 's/:.*//g;s/\s*<.*//g;s/\s*\[.*//')"

	leading=$(echo "$commands" | awk '{print $1}' | sort -u | paste -s -d' ')
	subcommands=""
	for cmd in $(echo "$commands" | awk '{print $1}' | sort -u); do
		subcmds=$(echo "$commands" | awk '($1 == "'$cmd'"){ print $2 }' | sort -u | paste -s -d' ')
		if (( ${#subcmds} > 0 )); then
			subcommands="${subcommands}\n\n\t\t$cmd)\n\t\t\tCOMPREPLY=( \$(compgen -W \"${subcmds}\" \${cur}) )\n\t\t\treturn 0\n\t\t;;"
		fi
	done;
	# Fix newlines
	subcommands="$(echo -e "$subcommands")"

	# Template
BASH_COMPLETION_TEMPLATE=$(cat <<EOF
_gxadmin() {
	local cur prev opts
	COMPREPLY=()
	cur="\${COMP_WORDS[COMP_CWORD]}"
	prev="\${COMP_WORDS[COMP_CWORD-1]}"

	local -r cmds="%s"

	case "\${prev}" in
		%s
	esac

	COMPREPLY=( \$(compgen -W "\${cmds}" -- \${cur}) )
	return 0
}

complete -F _gxadmin gxadmin
EOF
)

	printf "${BASH_COMPLETION_TEMPLATE}\n" "$leading" "$subcommands"
}
look_for() {
	group_name="$1"; shift
	query_name="$1"; shift

	# Check that FN exists
	fn="${group_name}_${query_name}"
	LC_ALL=C type $fn 2> /dev/null | grep -q 'function'

	if (( $? == 0 )); then
		$fn "$@";
	else
		usage ${group_name}
	fi
}
obtain_mutate() {
	query_name="$1"; shift

	fn="mutate_${query_name}"
	LC_ALL=C type $fn 2> /dev/null | grep -q 'function'

	if (( $? == 0 )); then
		$fn "$@";
	else
		export QUERY="ERROR"
	fi
}

mutate() {
	query_type="$1"; shift
	query_name="$1"; shift

	# We do not run this in a subshell because we need to "return" multiple things.
	obtain_mutate $query_name "$@"

	# TODO(hxr)
	if [[ "$QUERY" == "ERROR" ]]; then
		usage mutate
	fi

	query_tbl "$QUERY";
}
obtain_query() {
	query_name="$1"; shift

	fn="query_${query_name}"
	LC_ALL=C type $fn 2> /dev/null | grep -q 'function'

	if (( $? == 0 )); then
		$fn "$@";
	else
		export QUERY="ERROR"
	fi
}


query() {
	# do the thing zhu li
	query_type="$1"; shift
	query_name="$1"; shift

	# We do not run this in a subshell because we need to "return" multiple things.
	obtain_query $query_name "$@"

	# TODO(hxr)
	#ec=$?
	#if (( ec > 0 )); then
		#echo "$db_query"
		#exit 0
	#fi
	if [[ "$QUERY" == "ERROR" ]]; then
		usage query
	fi

	# Run the queries
	case "$query_type" in
		tsvquery )  query_tsv "$QUERY";;
		csvquery )  query_csv "$QUERY";;
		query    )  query_tbl "$QUERY";;
		iquery   )  query_influx "$QUERY" "$query_name" "$fields" "$tags";;
		# default
		*        )  usage "Error";;
	esac
}


local_funcs() {
	if [[ ! -f ${GXADMIN_SITE_SPECIFIC} ]]; then
		error "No local functions are defined in ${GXADMIN_SITE_SPECIFIC}"
		exit 1
	else
		# Load functions
		. ${GXADMIN_SITE_SPECIFIC}
	fi

	group_name="local"
	query_name="$1"; shift

	# Check that FN exists
	fn="${group_name}_${query_name}"
	LC_ALL=C type $fn 2> /dev/null | grep -q 'function'

	if (( $? == 0 )); then
		if [[ -z "${GXADMIN_BUGGER_OFF}" ]] && (( ($RANDOM % 25) == 0 )); then
			warning "Hey! It's great that you're using gxadmin! You should contribute these functions back :) Other people might find these useful, or could learn something from the code you've written, even if you think it is too specific to your site."
		fi
		$fn "$@";
	else
		usage ${group_name}
	fi
}
if (( $# == 0 )); then
	usage
fi

mode="$1"; shift

case "$mode" in
	*query ) query "$mode"   "$@" ;;
	config ) look_for config "$@" ;;
	filter ) look_for filter "$@" ;;
	galaxy ) look_for galaxy "$@" ;;
	report ) look_for report "$@" ;;
	local  ) local_funcs     "$@" ;;
	mutate ) mutate "$mode"  "$@" ;;
	uwsgi  ) look_for uwsgi  "$@" ;;
	meta   ) look_for meta   "$@" ;;

	# version commands
	version   ) version ;;
	-v        ) version ;;
	--version ) version ;;
	# help
	help      ) usage;;
	-h        ) usage;;
	--help    ) usage;;
	# anything else
	*         ) usage;;
esac
